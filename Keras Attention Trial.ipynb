{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "import time\n",
    "import theano\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "import six.moves.cPickle as pkl\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, LSTM, Lambda, merge\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D \n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import GRU, TimeDistributed, RepeatVector, Merge, TimeDistributedDense\n",
    "import h5py\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from attention_lstm import AttentionLSTM\n",
    "#import theano.tensor as T\n",
    "#import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "SEQUENCE_LENGTH = 45\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "BATCH_SIZE = 20\n",
    "OUTPUT_DIM = 512\n",
    "ANNOTATION_DIM = 512\n",
    "WORD_DIM = 512\n",
    "ANNOTATION_SIZE=196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "with open('features/flickr30kann_1000_p3.pkl', 'rb') as f:\n",
    "    d = pkl.load(f)\n",
    "#pkl.dump(d, open('features/flickr30kann_1000_p3.pkl', 'wb'), protocol=pkl.HIGHEST_PROTOCOL)\n",
    "vocab = d['vocab']\n",
    "word_to_index = d['word_to_index']\n",
    "index_to_word = d['index_to_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 44, 2452)\n",
      "(1000, 44, 2452)\n",
      "(1000, 196, 512)\n",
      "(1000, 512)\n"
     ]
    }
   ],
   "source": [
    "ins = d['INS']\n",
    "gts = d['GTS']\n",
    "ann_vec = d['AnnotationVectors']\n",
    "ann_vec = ann_vec.reshape(1000,196,512)\n",
    "z_mean = np.mean(ann_vec, axis=1)\n",
    "print(ins.shape)\n",
    "print(gts.shape)\n",
    "print(ann_vec.shape)\n",
    "print(z_mean.shape)\n",
    "VOCAB_COUNT = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 44, 2452)\n",
      "(100, 44, 2452)\n",
      "(899, 512)\n"
     ]
    }
   ],
   "source": [
    "ins_train = ins[1:900, :, :]\n",
    "gts_train = gts[1:900, :, :]\n",
    "ann_vec_train = ann_vec[1:900, :, :]\n",
    "z_mean_train = z_mean[1:900, :]\n",
    "\n",
    "ins_test = ins[900:, :, :]\n",
    "gts_test = gts[900, :, :]\n",
    "ann_vec_test = ann_vec[900:, :, :]\n",
    "z_mean_test = z_mean[900:, :]\n",
    "\n",
    "x_train = [ins_train, ann_vec_train, z_mean_train]\n",
    "x_test = [ins_test, ann_vec_test, z_mean_test]\n",
    "\n",
    "y_train = gts_train\n",
    "y_test = gts_test\n",
    "print(ins_train.shape)\n",
    "print(ins_test.shape)\n",
    "print(z_mean_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    print('Building Model')\n",
    "    x_inp = Input(shape=(SEQUENCE_LENGTH-1, VOCAB_COUNT))\n",
    "    z_inp = Input(shape=(ANNOTATION_SIZE, ANNOTATION_DIM,))\n",
    "    z_mean = Input(shape=(ANNOTATION_DIM,))\n",
    "    h_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)\n",
    "    c_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)\n",
    "    emb = Embedding(VOCAB_COUNT, WORD_DIM, input_length=SEQUENCE_LENGTH-1)(x_inp)\n",
    "    tdemb = TimeDistributed(Dense(OUTPUT_DIM))(x_inp)\n",
    "    print(h_Dense)\n",
    "    print(c_Dense)\n",
    "    #merge1 = merge([emb, h_Dense, c_Dense, z_inp], mode='concat', concat_axis=-1)\n",
    "    aLstm = AttentionLSTM(output_dim=WORD_DIM, z_dim=ANNOTATION_DIM, return_sequences=True)([tdemb, h_Dense, c_Dense, z_inp])\n",
    "    tdense = TimeDistributed(Dense(VOCAB_COUNT))(aLstm)\n",
    "    act = Activation('softmax')(tdense)\n",
    "    model = Model(input=[x_inp, z_inp, z_mean], output=act)\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    print('Adding Embedding')\n",
    "    model.add(Embedding(VOCAB_COUNT, EMBEDDING_SIZE, input_length=SEQUENCE_LENGTH-1))\n",
    "    print('Adding LSTM')\n",
    "    model.add(AttentionLSTM(EMBEDDING_SIZE, Z_DIM, return_sequences=True))\n",
    "    print('Adding TimeDistributed Dense')\n",
    "    model.add(TimeDistributed(Dense(EMBEDDING_SIZE)))\n",
    "    return model\n",
    "    model.add(Dense(VOCAB_COUNT, activation='softmax'))\n",
    "    '''\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model\n",
      "Softmax.0\n",
      "Softmax.0\n",
      "Elemwise{add,no_inplace}.0\n",
      "Sum{axis=[1], acc_dtype=float64}.0\n",
      "Shape.0\n",
      "input shapes [(None, 44, 512), (None, 512), (None, 512), (None, 196, 512)]\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 44, 2452)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           262656      input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           262656      input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 196, 512)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 44, 512)       1255936     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attentionlstm_1 (AttentionLSTM)  (None, 44, 512)       3677185     timedistributed_1[0][0]          \n",
      "                                                                   dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(None, 44, 2452)      1257876     attentionlstm_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 44, 2452)      0           timedistributed_2[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 6716309\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 99s - loss: 2.4658 - acc: 0.6252    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x932b1828>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([ins, ann_vec, z_mean], gts, batch_size=1, nb_epoch=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=20, nb_epoch=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = model.evaluate(x_test, y_test, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_processing(dataset):\n",
    "    allwords = Counter()\n",
    "    for item in dataset:\n",
    "        for sentence in item['sentences']:\n",
    "            allwords.update(sentence['tokens'])\n",
    "            \n",
    "    vocab = [k for k, v in allwords.items()]\n",
    "    vocab.insert(0, '<NULL>')\n",
    "    vocab.append('<START>')\n",
    "    vocab.append('<END>')\n",
    "    vocab.append('<UNK>')\n",
    "\n",
    "    word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "    index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "    return vocab, word_to_index, index_to_word\n",
    "\n",
    "def import_flickr8kdataset():\n",
    "    dataset = json.load(open('captions/dataset_flickr8k.json'))['images']\n",
    "    #reduced length to a 300 for testing\n",
    "    val_set = list(filter(lambda x: x['split'] == 'val', dataset))\n",
    "    train_set = list(filter(lambda x: x['split'] == 'train', dataset))\n",
    "    test_set = list(filter(lambda x: x['split'] == 'test', dataset))\n",
    "    return train_set[:800]+val_set[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(arr):\n",
    "    return np.asarray(arr, dtype=theano.config.floatX)\n",
    "\n",
    "#Prep Image uses an skimage transform\n",
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    # Resize so smallest dim = 224, preserving aspect ratio\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (224, w*224/h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*224/w, 224), preserve_range=True)\n",
    "\n",
    "    # Central crop to 224x224\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    \n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    \n",
    "    # Shuffle axes to c01\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert to BGR\n",
    "    im = im[::-1, :, :]\n",
    "\n",
    "    im = im - MEAN_VALUES\n",
    "    return rawim, floatX(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model():\n",
    "    model = Sequential()\n",
    "    print('Adding Embedding')\n",
    "    model.add(Embedding(VOCAB_COUNT, EMBEDDING_SIZE, input_length=SEQUENCE_LENGTH-1))\n",
    "    print('Adding LSTM')\n",
    "    model.add(LSTM(EMBEDDING_SIZE, return_sequences=False))\n",
    "    #print('Adding TimeDistributed Dense')\n",
    "    #model.add(TimeDistributed(Dense(EMBEDDING_SIZE)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = import_flickr8kdataset()\n",
    "# Currently testing it out\n",
    "dataset = [i for i in dataset[:100]]\n",
    "vocab,word_to_index, index_to_word = word_processing(dataset)\n",
    "#print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def process_images(dataset, coco=False, d_set=\"Flicker8k_Dataset\"):\n",
    "    ind_process = 1\n",
    "    total = len(dataset)\n",
    "    cnn_input = floatX(np.zeros((len(dataset), 3, 224, 224)))\n",
    "    rawim_input = []\n",
    "    sentences_tokens = []\n",
    "    for i, image in enumerate(dataset):\n",
    "        print \"ind_process %s total %s\" %(str(ind_process),str(total))\n",
    "        ind_process+=1\n",
    "        if coco:\n",
    "            fn = './coco/{}/{}'.format(image['filepath'], image['filename'])\n",
    "        else:\n",
    "            fn = d_set+'/{}'.format(image['filename'])\n",
    "        try:\n",
    "            im = plt.imread(fn)\n",
    "            rawim, cnn_input[i] = prep_image(im)\n",
    "            sentences_tokens.append(image['sentences'][0]['tokens'])\n",
    "            rawim_input.append(rawim)\n",
    "        except IOError:\n",
    "            continue\n",
    "    return rawim_input, cnn_input, sentences_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawim_array, cnnim_array, sentences_tokens = process_images(dataset, coco=False, d_set=\"Flicker8k_Dataset\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_image_partial_captions(images, captions, word_to_index, vocab_count, model):\n",
    "    a_images = []\n",
    "    a_captions = []\n",
    "    next_words = []\n",
    "    #vocab_size = len(vocab)\n",
    "    for ind, image in enumerate(images):\n",
    "        sentence = captions[ind]\n",
    "        partial_caption_ar = np.zeros(SEQUENCE_LENGTH-1, dtype=np.int)\n",
    "        \n",
    "        words = ['<START>'] + sentence + ['<END>']\n",
    "        assert len(words)<SEQUENCE_LENGTH\n",
    "        for i in range(len(words) - 1):\n",
    "            pc_copy = partial_caption_ar.copy()\n",
    "            if words[i] in word_to_index:\n",
    "                pc_copy[i] = word_to_index[words[i]]\n",
    "            else:\n",
    "                pc_copy[i] = word_to_index[\"<UNK>\"]\n",
    "            a_images.append(image)\n",
    "            a_captions.append(pc_copy)\n",
    "            #Generate next word output vector\n",
    "            next_word = words[i + 1]\n",
    "            if next_word in word_to_index:\n",
    "                next_word_index = word_to_index[next_word]\n",
    "            else:\n",
    "                next_word_index = word_to_index[\"<UNK>\"]\n",
    "            next_word_ar = np.zeros(vocab_count, dtype=np.int)\n",
    "            next_word_ar[next_word_index] = 1\n",
    "            next_words.append(next_word_ar)\n",
    "    v_i = np.array(a_images)\n",
    "    v_c = np.array(a_captions)\n",
    "    v_nw = np.array(next_words)\n",
    "    return v_i, v_c, v_nw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vocab_count = len(word_to_index)\n",
    "#print cnnim_array.shape\n",
    "#v_i, v_c, v_nw = gen_image_partial_captions(cnnim_array, sentences_tokens, word_to_index, vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_COUNT = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def process_cnn_features(dataset, model, coco=False, d_set=\"Flicker8k_Dataset\"):\n",
    "    ind_process = 1\n",
    "    total = len(dataset)\n",
    "    for chunk in chunks(dataset, 25):\n",
    "        cnn_input = floatX(np.zeros((len(chunk), 3, 224, 224)))\n",
    "        for i, image in enumerate(chunk):\n",
    "            print \"ind_process %s total %s\" %(str(ind_process),str(total))\n",
    "            ind_process+=1\n",
    "            if coco:\n",
    "                fn = './coco/{}/{}'.format(image['filepath'], image['filename'])\n",
    "            else:\n",
    "                fn = d_set+'/{}'.format(image['filename'])\n",
    "            try:\n",
    "                im = plt.imread(fn)\n",
    "                _, cnn_input[i] = prep_image(im)\n",
    "            except IOError:\n",
    "                continue\n",
    "        features = model.predict(cnn_input)\n",
    "        print \"Processing Features For Chunk\"\n",
    "        for i, image in enumerate(chunk):\n",
    "            image['cnn features'] = features[i]\n",
    "            \n",
    "def process_captions_array(dataset):\n",
    "    ind_process = 1\n",
    "    total = len(dataset)\n",
    "    for chunk in chunks(dataset, 25):\n",
    "        for i in chunk:\n",
    "            \n",
    "            image[\"captions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = VGG_16('weights/vgg16_weights.h5')\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, images, index_to_word, word_to_index):\n",
    "    for image in images:\n",
    "        caption = np.zeros(SEQUENCE_LENGTH - 1).reshape(1, SEQUENCE_LENGTH - 1)\n",
    "        print(caption.shape)\n",
    "        caption[0,0] = word_to_index[\"<START>\"]\n",
    "        count=0\n",
    "        sentence = []\n",
    "        a = image.reshape(1,3,224,224)\n",
    "        #a = np.array([image])\n",
    "        while True:\n",
    "            out = model.predict([a, caption])\n",
    "            index = out.argmax(-1)\n",
    "            index = index[0]\n",
    "            word = index_to_word[index]\n",
    "            sentence.append(word)\n",
    "            count+= 1\n",
    "            if count >= SEQUENCE_LENGTH - 1 or index == word_to_index[\"<END>\"]: #max caption length reach of '<eos>' encountered\n",
    "                break\n",
    "            caption[0,count] = index\n",
    "        sent_str = \" \".join(sentence)\n",
    "        print(\"The Oracle says : %s\" %sent_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnnim_list = []\n",
    "for i in cnnim_array:\n",
    "    cnnim_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(model, cnnim_list, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caption = np.zeros(SEQUENCE_LENGTH).reshape(1, SEQUENCE_LENGTH)\n",
    "caption[0] = word_to_index[\"#START#\"]\n",
    "print cnnim_array[0].shape\n",
    "t = np.array(cnnim_array[0])\n",
    "print t.shape\n",
    "out = model.predict([v_i, v_c])\n",
    "print out\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
