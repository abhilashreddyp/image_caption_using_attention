{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "import time\n",
    "import theano\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "import six.moves.cPickle as pkl\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D \n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import GRU, TimeDistributed, RepeatVector, Merge, TimeDistributedDense\n",
    "import h5py\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "SEQUENCE_LENGTH = 32\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token\n",
    "BATCH_SIZE = 200\n",
    "CNN_FEATURE_SIZE = 1000\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_processing(dataset):\n",
    "    allwords = Counter()\n",
    "    for item in dataset:\n",
    "        for sentence in item['sentences']:\n",
    "            allwords.update(sentence['tokens'])\n",
    "            \n",
    "    vocab = [k for k, v in allwords.items() if v >= 5]\n",
    "    vocab.insert(0, '#START#')\n",
    "    vocab.append('#UNK#')\n",
    "    vocab.append('#END#')\n",
    "\n",
    "    word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "    index_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "    return vocab, word_to_index, index_to_word\n",
    "\n",
    "def import_flickr8kdataset():\n",
    "    dataset = json.load(open('captions/dataset_flickr8k.json'))['images']\n",
    "    #reduced length to a 300 for testing\n",
    "    val_set = list(filter(lambda x: x['split'] == 'val', dataset))\n",
    "    train_set = list(filter(lambda x: x['split'] == 'train', dataset))\n",
    "    test_set = list(filter(lambda x: x['split'] == 'test', dataset))\n",
    "    return train_set[:800]+val_set[:200]\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "def floatX(arr):\n",
    "    return np.asarray(arr, dtype=theano.config.floatX)\n",
    "\n",
    "#Prep Image uses an skimage transform\n",
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    # Resize so smallest dim = 224, preserving aspect ratio\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (224, w*224//h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*224//w, 224), preserve_range=True)\n",
    "\n",
    "    # Central crop to 224x224\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    \n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    \n",
    "    # Shuffle axes to c01\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert to BGR\n",
    "    im = im[::-1, :, :]\n",
    "\n",
    "    im = im - MEAN_VALUES\n",
    "    return rawim, floatX(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "def language_model():\n",
    "    model = Sequential()\n",
    "    print('Adding Embedding')\n",
    "    model.add(Embedding(VOCAB_COUNT, EMBEDDING_SIZE, input_length=SEQUENCE_LENGTH-1))\n",
    "    print('Adding LSTM')\n",
    "    model.add(LSTM(EMBEDDING_SIZE, return_sequences=True))\n",
    "    print('Adding TimeDistributed Dense')\n",
    "    model.add(TimeDistributed(Dense(EMBEDDING_SIZE)))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = import_flickr8kdataset()\n",
    "# Currently testing it out\n",
    "dataset = [i for i in dataset[:100]]\n",
    "vocab,word_to_index, index_to_word = word_processing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def process_images(dataset, coco=False, d_set=\"Flicker8k_Dataset\"):\n",
    "    ind_process = 1\n",
    "    total = len(dataset)\n",
    "    cnn_input = floatX(np.zeros((len(dataset), 3, 224, 224)))\n",
    "    rawim_input = []\n",
    "    sentences_tokens = []\n",
    "    for i, image in enumerate(dataset):\n",
    "        #print(\"ind_process %s total %s\" %(str(ind_process),str(total)))\n",
    "        ind_process+=1\n",
    "        if coco:\n",
    "            fn = './coco/{}/{}'.format(image['filepath'], image['filename'])\n",
    "        else:\n",
    "            fn = d_set+'/{}'.format(image['filename'])\n",
    "        try:\n",
    "            im = plt.imread(fn)\n",
    "            rawim, cnn_input[i] = prep_image(im)\n",
    "            sentences_tokens.append(image['sentences'][0]['tokens'])\n",
    "            rawim_input.append(rawim)\n",
    "        except IOError:\n",
    "            continue\n",
    "    return rawim_input, cnn_input, sentences_tokens\n",
    "\n",
    "def process_cnn_features(dataset, model, coco=False, d_set=\"Flicker8k_Dataset\"):\n",
    "    ind_process = 1\n",
    "    total = len(dataset)\n",
    "    for chunk in chunks(dataset, 25):\n",
    "        cnn_input = floatX(np.zeros((len(chunk), 3, 224, 224)))\n",
    "        for i, image in enumerate(chunk):\n",
    "            #print(\"ind_process %s total %s\" %(str(ind_process),str(total)))\n",
    "            ind_process+=1\n",
    "            if coco:\n",
    "                fn = './coco/{}/{}'.format(image['filepath'], image['filename'])\n",
    "            else:\n",
    "                fn = d_set+'/{}'.format(image['filename'])\n",
    "            try:\n",
    "                im = plt.imread(fn)\n",
    "                _, cnn_input[i] = prep_image(im)\n",
    "            except IOError:\n",
    "                continue\n",
    "        features = model.predict(cnn_input)\n",
    "        print(features.shape)\n",
    "        print(features[0].shape)\n",
    "        print(\"Processing Features For Chunk\")\n",
    "        for i, image in enumerate(chunk):\n",
    "            image['cnn features'] = features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_process 1 total 100\n",
      "ind_process 2 total 100\n",
      "ind_process 3 total 100\n",
      "ind_process 4 total 100\n",
      "ind_process 5 total 100\n",
      "ind_process 6 total 100\n",
      "ind_process 7 total 100\n",
      "ind_process 8 total 100\n",
      "ind_process 9 total 100\n",
      "ind_process 10 total 100\n",
      "ind_process 11 total 100\n",
      "ind_process 12 total 100\n",
      "ind_process 13 total 100\n",
      "ind_process 14 total 100\n",
      "ind_process 15 total 100\n",
      "ind_process 16 total 100\n",
      "ind_process 17 total 100\n",
      "ind_process 18 total 100\n",
      "ind_process 19 total 100\n",
      "ind_process 20 total 100\n",
      "ind_process 21 total 100\n",
      "ind_process 22 total 100\n",
      "ind_process 23 total 100\n",
      "ind_process 24 total 100\n",
      "ind_process 25 total 100\n",
      "ind_process 26 total 100\n",
      "ind_process 27 total 100\n",
      "ind_process 28 total 100\n",
      "ind_process 29 total 100\n",
      "ind_process 30 total 100\n",
      "ind_process 31 total 100\n",
      "ind_process 32 total 100\n",
      "ind_process 33 total 100\n",
      "ind_process 34 total 100\n",
      "ind_process 35 total 100\n",
      "ind_process 36 total 100\n",
      "ind_process 37 total 100\n",
      "ind_process 38 total 100\n",
      "ind_process 39 total 100\n",
      "ind_process 40 total 100\n",
      "ind_process 41 total 100\n",
      "ind_process 42 total 100\n",
      "ind_process 43 total 100\n",
      "ind_process 44 total 100\n",
      "ind_process 45 total 100\n",
      "ind_process 46 total 100\n",
      "ind_process 47 total 100\n",
      "ind_process 48 total 100\n",
      "ind_process 49 total 100\n",
      "ind_process 50 total 100\n",
      "ind_process 51 total 100\n",
      "ind_process 52 total 100\n",
      "ind_process 53 total 100\n",
      "ind_process 54 total 100\n",
      "ind_process 55 total 100\n",
      "ind_process 56 total 100\n",
      "ind_process 57 total 100\n",
      "ind_process 58 total 100\n",
      "ind_process 59 total 100\n",
      "ind_process 60 total 100\n",
      "ind_process 61 total 100\n",
      "ind_process 62 total 100\n",
      "ind_process 63 total 100\n",
      "ind_process 64 total 100\n",
      "ind_process 65 total 100\n",
      "ind_process 66 total 100\n",
      "ind_process 67 total 100\n",
      "ind_process 68 total 100\n",
      "ind_process 69 total 100\n",
      "ind_process 70 total 100\n",
      "ind_process 71 total 100\n",
      "ind_process 72 total 100\n",
      "ind_process 73 total 100\n",
      "ind_process 74 total 100\n",
      "ind_process 75 total 100\n",
      "ind_process 76 total 100\n",
      "ind_process 77 total 100\n",
      "ind_process 78 total 100\n",
      "ind_process 79 total 100\n",
      "ind_process 80 total 100\n",
      "ind_process 81 total 100\n",
      "ind_process 82 total 100\n",
      "ind_process 83 total 100\n",
      "ind_process 84 total 100\n",
      "ind_process 85 total 100\n",
      "ind_process 86 total 100\n",
      "ind_process 87 total 100\n",
      "ind_process 88 total 100\n",
      "ind_process 89 total 100\n",
      "ind_process 90 total 100\n",
      "ind_process 91 total 100\n",
      "ind_process 92 total 100\n",
      "ind_process 93 total 100\n",
      "ind_process 94 total 100\n",
      "ind_process 95 total 100\n",
      "ind_process 96 total 100\n",
      "ind_process 97 total 100\n",
      "ind_process 98 total 100\n",
      "ind_process 99 total 100\n",
      "ind_process 100 total 100\n",
      "ind_process 1 total 100\n",
      "ind_process 2 total 100\n",
      "ind_process 3 total 100\n",
      "ind_process 4 total 100\n",
      "ind_process 5 total 100\n",
      "ind_process 6 total 100\n",
      "ind_process 7 total 100\n",
      "ind_process 8 total 100\n",
      "ind_process 9 total 100\n",
      "ind_process 10 total 100\n",
      "ind_process 11 total 100\n",
      "ind_process 12 total 100\n",
      "ind_process 13 total 100\n",
      "ind_process 14 total 100\n",
      "ind_process 15 total 100\n",
      "ind_process 16 total 100\n",
      "ind_process 17 total 100\n",
      "ind_process 18 total 100\n",
      "ind_process 19 total 100\n",
      "ind_process 20 total 100\n",
      "ind_process 21 total 100\n",
      "ind_process 22 total 100\n",
      "ind_process 23 total 100\n",
      "ind_process 24 total 100\n",
      "ind_process 25 total 100\n",
      "(25, 1000)\n",
      "(1000,)\n",
      "Processing Features For Chunk\n",
      "ind_process 26 total 100\n",
      "ind_process 27 total 100\n",
      "ind_process 28 total 100\n",
      "ind_process 29 total 100\n",
      "ind_process 30 total 100\n",
      "ind_process 31 total 100\n",
      "ind_process 32 total 100\n",
      "ind_process 33 total 100\n",
      "ind_process 34 total 100\n",
      "ind_process 35 total 100\n",
      "ind_process 36 total 100\n",
      "ind_process 37 total 100\n",
      "ind_process 38 total 100\n",
      "ind_process 39 total 100\n",
      "ind_process 40 total 100\n",
      "ind_process 41 total 100\n",
      "ind_process 42 total 100\n",
      "ind_process 43 total 100\n",
      "ind_process 44 total 100\n",
      "ind_process 45 total 100\n",
      "ind_process 46 total 100\n",
      "ind_process 47 total 100\n",
      "ind_process 48 total 100\n",
      "ind_process 49 total 100\n",
      "ind_process 50 total 100\n",
      "(25, 1000)\n",
      "(1000,)\n",
      "Processing Features For Chunk\n",
      "ind_process 51 total 100\n",
      "ind_process 52 total 100\n",
      "ind_process 53 total 100\n",
      "ind_process 54 total 100\n",
      "ind_process 55 total 100\n",
      "ind_process 56 total 100\n",
      "ind_process 57 total 100\n",
      "ind_process 58 total 100\n",
      "ind_process 59 total 100\n",
      "ind_process 60 total 100\n",
      "ind_process 61 total 100\n",
      "ind_process 62 total 100\n",
      "ind_process 63 total 100\n",
      "ind_process 64 total 100\n",
      "ind_process 65 total 100\n",
      "ind_process 66 total 100\n",
      "ind_process 67 total 100\n",
      "ind_process 68 total 100\n",
      "ind_process 69 total 100\n",
      "ind_process 70 total 100\n",
      "ind_process 71 total 100\n",
      "ind_process 72 total 100\n",
      "ind_process 73 total 100\n",
      "ind_process 74 total 100\n",
      "ind_process 75 total 100\n",
      "(25, 1000)\n",
      "(1000,)\n",
      "Processing Features For Chunk\n",
      "ind_process 76 total 100\n",
      "ind_process 77 total 100\n",
      "ind_process 78 total 100\n",
      "ind_process 79 total 100\n",
      "ind_process 80 total 100\n",
      "ind_process 81 total 100\n",
      "ind_process 82 total 100\n",
      "ind_process 83 total 100\n",
      "ind_process 84 total 100\n",
      "ind_process 85 total 100\n",
      "ind_process 86 total 100\n",
      "ind_process 87 total 100\n",
      "ind_process 88 total 100\n",
      "ind_process 89 total 100\n",
      "ind_process 90 total 100\n",
      "ind_process 91 total 100\n",
      "ind_process 92 total 100\n",
      "ind_process 93 total 100\n",
      "ind_process 94 total 100\n",
      "ind_process 95 total 100\n",
      "ind_process 96 total 100\n",
      "ind_process 97 total 100\n",
      "ind_process 98 total 100\n",
      "ind_process 99 total 100\n",
      "ind_process 100 total 100\n",
      "(25, 1000)\n",
      "(1000,)\n",
      "Processing Features For Chunk\n"
     ]
    }
   ],
   "source": [
    "image_model = VGG_16('weights/vgg16_weights.h5')\n",
    "rawim_array, cnnim_array, sentences_tokens = process_images(dataset, coco=False, d_set=\"Flicker8k_Dataset\")\n",
    "process_cnn_features(dataset, image_model, False, \"Flicker8k_Dataset\")\n",
    "pkl.dump(dataset, open('flickr8k_800_200_with_cnn_features.pkl','wb'), protocol=pkl.HIGHEST_PROTOCOL)\n",
    "#get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_image_partial_captions(images, captions, word_to_index, vocab_count):\n",
    "    a_features = []\n",
    "    a_captions = []\n",
    "    next_words = []\n",
    "    #vocab_size = len(vocab)\n",
    "    for ind, image in enumerate(dataset):\n",
    "        sentence = captions[ind]\n",
    "        partial_caption_ar = np.zeros(SEQUENCE_LENGTH-1, dtype=np.int)\n",
    "        \n",
    "        words = ['#START#'] + sentence + ['#END#']\n",
    "        assert len(words)<SEQUENCE_LENGTH\n",
    "        for i in range(len(words) - 1):\n",
    "            pc_copy = partial_caption_ar.copy()\n",
    "            if words[i] in word_to_index:\n",
    "                pc_copy[i] = word_to_index[words[i]]\n",
    "            else:\n",
    "                pc_copy[i] = word_to_index[\"#UNK#\"]\n",
    "            a_features.append(image['cnn features'])\n",
    "            a_captions.append(pc_copy)\n",
    "            #Generate next word output vector\n",
    "            next_word = words[i + 1]\n",
    "            if next_word in word_to_index:\n",
    "                next_word_index = word_to_index[next_word]\n",
    "            else:\n",
    "                next_word_index = word_to_index[\"#UNK#\"]\n",
    "            next_word_ar = np.zeros(vocab_count, dtype=np.int)\n",
    "            next_word_ar[next_word_index] = 1\n",
    "            next_words.append(next_word_ar)\n",
    "    v_i = np.array(a_features)\n",
    "    print(v_i.shape)\n",
    "    v_c = np.array(a_captions)\n",
    "    v_nw = np.array(next_words)\n",
    "    return v_i, v_c, v_nw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 224, 224)\n",
      "(1274, 1000)\n"
     ]
    }
   ],
   "source": [
    "vocab_count = len(word_to_index)\n",
    "print(cnnim_array.shape)\n",
    "v_i, v_c, v_nw = gen_image_partial_captions(cnnim_array, sentences_tokens, word_to_index, vocab_count)\n",
    "\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "VOCAB_COUNT = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(weights_path):\n",
    "    #image_model = VGG_16(weights_path)\n",
    "    #image_model.add(Dense(EMBEDDING_SIZE, activation='tanh'))\n",
    "    #image_model.add(RepeatVector(SEQUENCE_LENGTH-1))\n",
    "    print('Built Image Model')\n",
    "    print('Building Language Model')\n",
    "    image_model = Sequential()\n",
    "    image_model.add(Dense(CNN_FEATURE_SIZE, input_dim=CNN_FEATURE_SIZE))\n",
    "    image_model.add(RepeatVector(SEQUENCE_LENGTH-1))\n",
    "    lang_model = language_model()\n",
    "    #model = lang_model\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, lang_model], mode='concat',  concat_axis=-1))\n",
    "    #model.add(Merge([image_model, lang_model], mode='concat',  concat_axis=-1))\n",
    "    model.add(LSTM(EMBEDDING_SIZE, return_sequences=False))\n",
    "    #print(vocab_size)\n",
    "    model.add(Dense(VOCAB_COUNT, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def predict(model, images, index_to_word, word_to_index):\n",
    "    for ind, image in enumerate(dataset):\n",
    "        caption = np.zeros(SEQUENCE_LENGTH - 1).reshape(1, SEQUENCE_LENGTH - 1)\n",
    "        #print(caption.shape)\n",
    "        caption[0,0] = 0\n",
    "        count=0\n",
    "        sentence = []\n",
    "        #a = image.reshape(1,3,224,224)\n",
    "        #a = np.array([image])\n",
    "        f = image['cnn features'].reshape(1, CNN_FEATURE_SIZE)\n",
    "        while True:\n",
    "            out = model.predict([f, caption])\n",
    "            index = out.argmax(-1)\n",
    "            #print(index)\n",
    "            index = index[0]\n",
    "            word = index_to_word[index]\n",
    "            sentence.append(word)\n",
    "            count+= 1\n",
    "            if count >= SEQUENCE_LENGTH - 1 or index == word_to_index[\"#END#\"]: #max caption length reach of '<eos>' encountered\n",
    "                break\n",
    "            caption[0,count] = index\n",
    "        sent_str = \" \".join(sentence)\n",
    "        print(\"The Oracle says : %s\" %sent_str)\n",
    "\n",
    "# In[158]:\n",
    "\n",
    "def train():\n",
    "    model=build_model('weights/vgg16_weights.h5')\n",
    "    print('Built model.')\n",
    "    print('Compiling Now')\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print('Fitting Now')\n",
    "    model.fit([v_i, v_c], v_nw, batch_size=BATCH_SIZE, nb_epoch=100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Image Model\n",
      "Building Language Model\n",
      "Adding Embedding\n",
      "Adding LSTM\n",
      "Adding TimeDistributed Dense\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_13 (Dense)                 (None, 1000)          1001000     dense_input_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_3 (RepeatVector)    (None, 31, 1000)      0           dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 31, 256)       51456       embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 31, 256)       525312      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(None, 31, 256)       65792       lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 256)           1549312     merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 201)           51657       lstm_6[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 3244529\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Built model.\n",
      "Compiling Now\n",
      "Fitting Now\n",
      "Epoch 1/100\n",
      "1274/1274 [==============================] - 10s - loss: 4.5944 - acc: 0.1272    \n",
      "Epoch 2/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.9395 - acc: 0.1750    \n",
      "Epoch 3/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8696 - acc: 0.1562    \n",
      "Epoch 4/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8548 - acc: 0.1743    \n",
      "Epoch 5/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8399 - acc: 0.1680    \n",
      "Epoch 6/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8253 - acc: 0.1837    \n",
      "Epoch 7/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8279 - acc: 0.1633    \n",
      "Epoch 8/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8097 - acc: 0.1845    \n",
      "Epoch 9/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.8006 - acc: 0.1860    \n",
      "Epoch 10/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.7951 - acc: 0.1860    \n",
      "Epoch 11/100\n",
      "1274/1274 [==============================] - 12s - loss: 3.7785 - acc: 0.1907    \n",
      "Epoch 12/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.7615 - acc: 0.1884    \n",
      "Epoch 13/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.7433 - acc: 0.1884    \n",
      "Epoch 14/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.7227 - acc: 0.1915    \n",
      "Epoch 15/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.6987 - acc: 0.1970    \n",
      "Epoch 16/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.6681 - acc: 0.1907    \n",
      "Epoch 17/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.6287 - acc: 0.1939    \n",
      "Epoch 18/100\n",
      "1274/1274 [==============================] - 11s - loss: 3.6011 - acc: 0.1923    \n",
      "Epoch 19/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.5780 - acc: 0.1947     \n",
      "Epoch 20/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.5247 - acc: 0.2009     \n",
      "Epoch 21/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.4808 - acc: 0.1986     \n",
      "Epoch 22/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.4389 - acc: 0.1962     \n",
      "Epoch 23/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.3973 - acc: 0.1986     \n",
      "Epoch 24/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.3543 - acc: 0.1923     \n",
      "Epoch 25/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.3291 - acc: 0.1962     \n",
      "Epoch 26/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.2719 - acc: 0.1962     \n",
      "Epoch 27/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.2347 - acc: 0.1939     \n",
      "Epoch 28/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.2020 - acc: 0.1978     \n",
      "Epoch 29/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.1711 - acc: 0.1931     \n",
      "Epoch 30/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.1236 - acc: 0.1986     \n",
      "Epoch 31/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.0745 - acc: 0.1939     \n",
      "Epoch 32/100\n",
      "1274/1274 [==============================] - 9s - loss: 3.0229 - acc: 0.2033     \n",
      "Epoch 33/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.9333 - acc: 0.2214     \n",
      "Epoch 34/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.8889 - acc: 0.2292     \n",
      "Epoch 35/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.8079 - acc: 0.2465     \n",
      "Epoch 36/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.7583 - acc: 0.2386     \n",
      "Epoch 37/100\n",
      "1274/1274 [==============================] - 10s - loss: 2.6575 - acc: 0.2763    \n",
      "Epoch 38/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.5307 - acc: 0.2959     \n",
      "Epoch 39/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.4346 - acc: 0.3250     \n",
      "Epoch 40/100\n",
      "1274/1274 [==============================] - 10s - loss: 2.3493 - acc: 0.3352    \n",
      "Epoch 41/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.2596 - acc: 0.3642     \n",
      "Epoch 42/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.1905 - acc: 0.3736     \n",
      "Epoch 43/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.0994 - acc: 0.3956     \n",
      "Epoch 44/100\n",
      "1274/1274 [==============================] - 9s - loss: 2.0444 - acc: 0.4192     \n",
      "Epoch 45/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.9741 - acc: 0.4349     \n",
      "Epoch 46/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.8799 - acc: 0.4458     \n",
      "Epoch 47/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.8296 - acc: 0.4513     \n",
      "Epoch 48/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.7240 - acc: 0.5000     \n",
      "Epoch 49/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.6673 - acc: 0.5086     \n",
      "Epoch 50/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.5934 - acc: 0.5298     \n",
      "Epoch 51/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.5312 - acc: 0.5416     \n",
      "Epoch 52/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.4635 - acc: 0.5691     \n",
      "Epoch 53/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.4308 - acc: 0.5659     \n",
      "Epoch 54/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.4102 - acc: 0.5706     \n",
      "Epoch 55/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.3229 - acc: 0.6036     \n",
      "Epoch 56/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.2582 - acc: 0.6193     \n",
      "Epoch 57/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.2113 - acc: 0.6436     \n",
      "Epoch 58/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.1577 - acc: 0.6672     \n",
      "Epoch 59/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.1440 - acc: 0.6570     \n",
      "Epoch 60/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.1024 - acc: 0.6766     \n",
      "Epoch 61/100\n",
      "1274/1274 [==============================] - 9s - loss: 1.0335 - acc: 0.7119     \n",
      "Epoch 62/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.9820 - acc: 0.7119     \n",
      "Epoch 63/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.9287 - acc: 0.7394     \n",
      "Epoch 64/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.8906 - acc: 0.7739     \n",
      "Epoch 65/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.8444 - acc: 0.7818     \n",
      "Epoch 66/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.8423 - acc: 0.7692     \n",
      "Epoch 67/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.8839 - acc: 0.7661     \n",
      "Epoch 68/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.7982 - acc: 0.7896     \n",
      "Epoch 69/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.7778 - acc: 0.7991     \n",
      "Epoch 70/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.7176 - acc: 0.8344     \n",
      "Epoch 71/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.6897 - acc: 0.8305     \n",
      "Epoch 72/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.6483 - acc: 0.8564     \n",
      "Epoch 73/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.5989 - acc: 0.8650     \n",
      "Epoch 74/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.5820 - acc: 0.8783     \n",
      "Epoch 75/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.5615 - acc: 0.8830     \n",
      "Epoch 76/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.5302 - acc: 0.8987     \n",
      "Epoch 77/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.5034 - acc: 0.9042    \n",
      "Epoch 78/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.4819 - acc: 0.9199    \n",
      "Epoch 79/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.4559 - acc: 0.9270    \n",
      "Epoch 80/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.4292 - acc: 0.9396    \n",
      "Epoch 81/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.4068 - acc: 0.9451    \n",
      "Epoch 82/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.3932 - acc: 0.9427    \n",
      "Epoch 83/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.3730 - acc: 0.9482    \n",
      "Epoch 84/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.3692 - acc: 0.9458    \n",
      "Epoch 85/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.3678 - acc: 0.9466     \n",
      "Epoch 86/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.3407 - acc: 0.9615    \n",
      "Epoch 87/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.3259 - acc: 0.9592     \n",
      "Epoch 88/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.3080 - acc: 0.9608    \n",
      "Epoch 89/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2907 - acc: 0.9702     \n",
      "Epoch 90/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2736 - acc: 0.9702     \n",
      "Epoch 91/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2598 - acc: 0.9772     \n",
      "Epoch 92/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2585 - acc: 0.9796     \n",
      "Epoch 93/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2505 - acc: 0.9765     \n",
      "Epoch 94/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2490 - acc: 0.9725     \n",
      "Epoch 95/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.2439 - acc: 0.9733    \n",
      "Epoch 96/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2290 - acc: 0.9788     \n",
      "Epoch 97/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2242 - acc: 0.9749     \n",
      "Epoch 98/100\n",
      "1274/1274 [==============================] - 10s - loss: 0.2185 - acc: 0.9757    \n",
      "Epoch 99/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2152 - acc: 0.9788     \n",
      "Epoch 100/100\n",
      "1274/1274 [==============================] - 9s - loss: 0.2257 - acc: 0.9733     \n",
      "Trained on 100 images, saved weights to weights_20161207-215453.hf5\n",
      "(100, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "model = train()\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_name = 'weights_'+timestr+'.hf5'\n",
    "#model.save_weights(file_name)\n",
    "print('Trained on %s images, saved weights to %s'%(len(cnnim_array), file_name))\n",
    "print(cnnim_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Oracle says : a black black black black black black black black black black black black dog dog dog black dog dog dog dog dog dog dog dog dog black black black black black\n",
      "The Oracle says : a little little man man man man man man man man little man little #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a brown brown brown brown dog dog dog dog dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a brown brown dog dog dog dog dog dog dog dog dog dog dog dog dog dog brown brown dog dog brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a black black black black black black black black black black black black black black black black black black white white white white white beach beach beach white black black black\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a man man man man man man man man man man man man man purple man man purple man lady lady lady lady lady lady lady lady lady lady lady lady\n",
      "The Oracle says : a boy boy boy boy boy boy man man man man man man blond man blond man man man man man man man man blond blond blond blond blond blond blond\n",
      "The Oracle says : a girl girl girl girl girl girl girl girl girl #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# black #END#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a blue #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man\n",
      "The Oracle says : a dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog brown brown brown dog dog dog dog dog dog dog dog\n",
      "The Oracle says : a little little little little little little little little little little little little little little little little little #UNK# #UNK# #UNK# walking #UNK# #UNK# #UNK# man man man man man man\n",
      "The Oracle says : a blond blond blond blond blond blond blond blond blond blond blond blond blond blond trail blond along blond along trail blond blond along along along along along trail trail trail\n",
      "The Oracle says : a family family #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# has has #UNK# has has has\n",
      "The Oracle says : man in in #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : three children in #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy #UNK# boy shirt shirt shirt shirt #END#\n",
      "The Oracle says : a brown brown brown brown dog dog dog dog dog dog dog dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : two dogs dogs dogs dogs dogs dogs dogs dogs dogs dogs dogs dogs dogs dogs #UNK# dogs #UNK# dogs #UNK# dogs #UNK# #UNK# #UNK# dogs #UNK# #UNK# dogs dogs dogs dogs\n",
      "The Oracle says : a baseball boy #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# baseball boy boy boy boy boy boy boy boy boy boy\n",
      "The Oracle says : a brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a group group group #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy #UNK# #UNK# #UNK# brown #END#\n",
      "The Oracle says : a group group group boy boy boy #UNK# #UNK# #UNK# boy boy boy boy boy boy boy boy #END#\n",
      "The Oracle says : a woman woman woman woman man man man man man man man man man man man man man group group group blond group blond blond blond blond blond blond blond blond\n",
      "The Oracle says : a snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder snowboarder #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a group group group #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a black black black black black black black black black black black black black black black black black black black black white white white white white white jumping jumping jumping jumping\n",
      "The Oracle says : a girl girl girl girl girl girl #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man little little little little little little little\n",
      "The Oracle says : a dog dog dog dog dog dog dog dog dog #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# black dog dog black black black\n",
      "The Oracle says : a group group group over over over #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a #UNK# #UNK# man man man man man man man man man man man along man man man little along along man man little brown along along along along along along\n",
      "The Oracle says : a car car #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# race #UNK#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a man man man man man man #UNK# man #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# dogs dogs man man\n",
      "The Oracle says : a boy boy boy boy man blond man man man man blond man dog dog dog dog #UNK# #UNK# #END#\n",
      "The Oracle says : two blond blond blond blond blond blond blond blond blond blond blond blond blond blond blond blond blond #UNK# brown #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown\n",
      "The Oracle says : four #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man\n",
      "The Oracle says : a dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a child #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# blond\n",
      "The Oracle says : a brown brown brown brown dog dog dog dog dog dog dog dog dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a man man man man man #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man dogs man\n",
      "The Oracle says : a little little little little little little little little little little little little little little little tree little little tree little #UNK# #UNK# #UNK# #UNK# little #UNK# #UNK# little #UNK# little\n",
      "The Oracle says : a group group group #UNK# group over #END#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man\n",
      "The Oracle says : a white dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog\n",
      "The Oracle says : a car race car car wet wet wet wet wet wet wet wet wet wet wet wet wet wet wet wet #UNK# wet #UNK# #UNK# wet #UNK# wet #UNK# #UNK# #UNK#\n",
      "The Oracle says : a person person person person person man man man man person person person person man man man person person person person person person person person person person person person person person\n",
      "The Oracle says : a brown brown brown dog dog dog dog dog dog dog dog dog dog dog brown dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a man man man man man man man walking walking man walking black walking walking walking walking walking walking walking walking walking walking walking walking walking walking walking walking walking walking\n",
      "The Oracle says : a brown brown brown brown brown brown dog dog dog brown brown brown brown brown brown brown brown brown #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a woman #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man man man man man man man man man man man man man man man\n",
      "The Oracle says : a man man man man #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man\n",
      "The Oracle says : a boy boy boy rock rock rock rock rock rock rock rock rock rock rock rock rock rock rock rock rock rock something something something something something something something something something\n",
      "The Oracle says : a brown brown brown brown dog dog dog dog dog dog dog dog dog dog brown dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man man\n",
      "The Oracle says : a #UNK# #UNK# dog dog man man man man #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# dog dog man man man man man man man man\n",
      "The Oracle says : two black black black black black black black black black black black black black jumping jumping black jumping jumping yellow tennis tennis tennis tennis tennis tennis tennis tennis yellow yellow yellow\n",
      "The Oracle says : an #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : an #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man\n",
      "The Oracle says : the men men are climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing climbing\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# group group group group group group\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man man\n",
      "The Oracle says : a man man man man child shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt shirt #UNK# #UNK# #UNK# shirt #UNK# #UNK# #UNK# shirt shirt black\n",
      "The Oracle says : a man man #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# black black black black black\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# black along black along along #END#\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a man man dog dog dog dog dog dog dog black black brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown dogs dogs\n",
      "The Oracle says : a brown brown brown brown brown dog dog brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man man man\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy brown #END#\n",
      "The Oracle says : a black black black black black black black black black black black black black black black dog dog dog dog dog dog black brown carrying carrying carrying carrying carrying carrying carrying\n",
      "The Oracle says : a group group group #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# has has #UNK# has has #UNK# has has has has #UNK# has has has has #END#\n",
      "The Oracle says : a man man bike bike dirt dirt dirt dirt dirt dirt dirt dirt dirt dirt dirt dirt #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# dirt dirt dirt dirt dirt\n",
      "The Oracle says : there are women women women #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a group group group #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : high #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : a man man man soccer soccer soccer soccer soccer soccer man man soccer man man soccer #UNK# #UNK# #UNK# soccer #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# man man man man man\n",
      "The Oracle says : a boy boy boy man man man man man man man blond blond #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# race race race race #UNK# race #UNK# race #UNK# #UNK# race #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# car car car car car man man\n",
      "The Oracle says : a girl girl girl girl girl girl girl girl girl girl girl girl girl #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# girl girl girl girl girl girl girl girl\n",
      "The Oracle says : a man man man man man man man man man man man man man man man man man lady lady lady lady lady lady lady lady lady lady lady lady lady\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# shorts #UNK# shorts #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown #END#\n",
      "The Oracle says : a #UNK# brown dog dog dog dog dog dog brown brown brown wet wet wet wet wet #UNK# #UNK# #UNK# wet #UNK# #UNK# #UNK# wet #UNK# brown brown brown brown brown\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #END#\n",
      "The Oracle says : a little little little little little little little little little little little little little little little #UNK# #UNK# #UNK# #UNK# little #UNK# #UNK# #UNK# #UNK# little #UNK# #UNK# little little little\n",
      "The Oracle says : a lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady lady\n",
      "The Oracle says : a boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy #UNK# #UNK# #UNK# #UNK# boy boy blond blond blond blond blond blond blond blond\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK#\n",
      "The Oracle says : two brown brown brown brown brown brown brown brown brown brown brown #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# brown brown brown brown brown brown brown brown\n",
      "The Oracle says : a boy boy boy #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# #UNK# boy #UNK# baseball baseball baseball boy boy boy boy boy boy boy boy boy\n"
     ]
    }
   ],
   "source": [
    "predict(model, cnnim_array, index_to_word, word_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
