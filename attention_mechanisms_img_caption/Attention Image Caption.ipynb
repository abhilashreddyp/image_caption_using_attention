{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import flickr30k\n",
    "import numpy as numpy\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... loaded train\n",
      "... loaded test\n",
      "... loaded dev\n"
     ]
    }
   ],
   "source": [
    "train, valid, test, worddict = flickr30k.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some useful shorthands\n",
    "def tanh(x):\n",
    "    return tensor.tanh(x)\n",
    "\n",
    "def rectifier(x):\n",
    "    return tensor.maximum(0., x)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index 0 and 1 always code for the end of sentence and unknown token\n",
    "word_idict = dict()\n",
    "for kk, vv in worddict.iteritems():\n",
    "    word_idict[vv] = kk\n",
    "word_idict[0] = '<eos>'\n",
    "word_idict[1] = 'UNK'\n",
    "\n",
    "\n",
    "layers = {'ff': ('param_init_fflayer', 'fflayer'),\n",
    "          'lstm': ('param_init_lstm', 'lstm_layer'),\n",
    "          'lstm_cond': ('param_init_lstm_cond', 'lstm_cond_layer'),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prefix-appended name\n",
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)\n",
    "\n",
    "# initialize Theano shared variables according to the initial parameters\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.iteritems():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return (eval(fns[0]), eval(fns[1]))\n",
    "\n",
    "# feedforward layer: affine transformation + point-wise nonlinearity\n",
    "def param_init_fflayer(options, params, prefix='ff', nin=None, nout=None):\n",
    "    if nin is None:\n",
    "        nin = options['dim_proj']\n",
    "    if nout is None:\n",
    "        nout = options['dim_proj']\n",
    "    params[_p(prefix, 'W')] = norm_weight(nin, nout, scale=0.01)\n",
    "    params[_p(prefix, 'b')] = numpy.zeros((nout,)).astype('float32')\n",
    "\n",
    "    return params\n",
    "\n",
    "def fflayer(tparams, state_below, options, prefix='rconv', activ='lambda x: tensor.tanh(x)', **kwargs):\n",
    "    return eval(activ)(tensor.dot(state_below, tparams[_p(prefix,'W')])+tparams[_p(prefix,'b')])\n",
    "\n",
    "\n",
    "# Conditional LSTM layer with Attention\n",
    "def param_init_lstm_cond(options, params, prefix='lstm_cond', nin=None, dim=None, dimctx=None):\n",
    "    if nin is None:\n",
    "        nin = options['dim']\n",
    "    if dim is None:\n",
    "        dim = options['dim']\n",
    "    if dimctx is None:\n",
    "        dimctx = options['dim']\n",
    "    # input to LSTM, similar to the above, we stack the matricies for compactness, do one\n",
    "    # dot product, and use the slice function below to get the activations for each \"gate\"\n",
    "    W = numpy.concatenate([norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim)], axis=1)\n",
    "    params[_p(prefix,'W')] = W\n",
    "\n",
    "    # LSTM to LSTM\n",
    "    U = numpy.concatenate([ortho_weight(dim),\n",
    "                           ortho_weight(dim),\n",
    "                           ortho_weight(dim),\n",
    "                           ortho_weight(dim)], axis=1)\n",
    "    params[_p(prefix,'U')] = U\n",
    "\n",
    "    # bias to LSTM\n",
    "    params[_p(prefix,'b')] = numpy.zeros((4 * dim,)).astype('float32')\n",
    "\n",
    "    # context to LSTM\n",
    "    Wc = norm_weight(dimctx,dim*4)\n",
    "    params[_p(prefix,'Wc')] = Wc\n",
    "\n",
    "    # attention: context -> hidden\n",
    "    Wc_att = norm_weight(dimctx, ortho=False)\n",
    "    params[_p(prefix,'Wc_att')] = Wc_att\n",
    "\n",
    "    # attention: LSTM -> hidden\n",
    "    Wd_att = norm_weight(dim,dimctx)\n",
    "    params[_p(prefix,'Wd_att')] = Wd_att\n",
    "\n",
    "    # attention: hidden bias\n",
    "    b_att = numpy.zeros((dimctx,)).astype('float32')\n",
    "    params[_p(prefix,'b_att')] = b_att\n",
    "\n",
    "    # attention:\n",
    "    U_att = norm_weight(dimctx,1)\n",
    "    params[_p(prefix,'U_att')] = U_att\n",
    "    c_att = numpy.zeros((1,)).astype('float32')\n",
    "    params[_p(prefix, 'c_tt')] = c_att\n",
    "\n",
    "    return params\n",
    "\n",
    "def lstm_cond_layer(tparams, state_below, options, prefix='lstm',\n",
    "                    mask=None, context=None, one_step=False,\n",
    "                    init_memory=None, init_state=None,\n",
    "                    trng=None, use_noise=None, sampling=True,\n",
    "                    argmax=False, **kwargs):\n",
    "\n",
    "    assert context, 'Context must be provided'\n",
    "\n",
    "    if one_step:\n",
    "        assert init_memory, 'previous memory must be provided'\n",
    "        assert init_state, 'previous state must be provided'\n",
    "\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    # mask\n",
    "    if mask is None:\n",
    "        mask = tensor.alloc(1., state_below.shape[0], 1)\n",
    "\n",
    "    # infer lstm dimension\n",
    "    dim = tparams[_p(prefix, 'U')].shape[0]\n",
    "\n",
    "    # initial/previous state\n",
    "    if init_state is None:\n",
    "        init_state = tensor.alloc(0., n_samples, dim)\n",
    "    # initial/previous memory\n",
    "    if init_memory is None:\n",
    "        init_memory = tensor.alloc(0., n_samples, dim)\n",
    "\n",
    "    # projected context\n",
    "    pctx_ = tensor.dot(context, tparams[_p(prefix,'Wc_att')]) + tparams[_p(prefix, 'b_att')]\n",
    "    if options['n_layers_att'] > 1:\n",
    "        for lidx in xrange(1, options['n_layers_att']):\n",
    "            pctx_ = tensor.dot(pctx_, tparams[_p(prefix,'W_att_%d'%lidx)])+tparams[_p(prefix, 'b_att_%d'%lidx)]\n",
    "            # note to self: this used to be options['n_layers_att'] - 1, so no extra non-linearity if n_layers_att < 3\n",
    "            if lidx < options['n_layers_att']:\n",
    "                pctx_ = tanh(pctx_)\n",
    "\n",
    "    # projected x\n",
    "    # state_below is timesteps*num samples by d in training (TODO change to notation of paper)\n",
    "    # this is n * d during sampling\n",
    "    state_below = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]\n",
    "\n",
    "    # additional parameters for stochastic hard attention\n",
    "    if options['attn_type'] == 'stochastic':\n",
    "        # temperature for softmax\n",
    "        temperature = options.get(\"temperature\", 1)\n",
    "        # [see (Section 4.1): Stochastic \"Hard\" Attention]\n",
    "        semi_sampling_p = options.get(\"semi_sampling_p\", 0.5)\n",
    "        temperature_c = theano.shared(numpy.float32(temperature), name='temperature_c')\n",
    "        h_sampling_mask = trng.binomial((1,), p=semi_sampling_p, n=1, dtype=theano.config.floatX).sum()\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n*dim:(n+1)*dim]\n",
    "        return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "    def _step(m_, x_, h_, c_, a_, as_, ct_, pctx_, dp_=None, dp_att_=None):\n",
    "        \"\"\" Each variable is one time slice of the LSTM\n",
    "        m_ - (mask), x_- (previous word), h_- (hidden state), c_- (lstm memory),\n",
    "        a_ - (alpha distribution [eq (5)]), as_- (sample from alpha dist), ct_- (context), \n",
    "        pctx_ (projected context), dp_/dp_att_ (dropout masks)\n",
    "        \"\"\"\n",
    "        # attention computation\n",
    "        # [described in  equations (4), (5), (6) in\n",
    "        # section \"3.1.2 Decoder: Long Short Term Memory Network]\n",
    "        pstate_ = tensor.dot(h_, tparams[_p(prefix,'Wd_att')])\n",
    "        pctx_ = pctx_ + pstate_[:,None,:]\n",
    "        pctx_list = []\n",
    "        pctx_list.append(pctx_)\n",
    "        pctx_ = tanh(pctx_)\n",
    "        alpha = tensor.dot(pctx_, tparams[_p(prefix,'U_att')])+tparams[_p(prefix, 'c_tt')]\n",
    "        alpha_pre = alpha\n",
    "        alpha_shp = alpha.shape\n",
    "\n",
    "        if options['attn_type'] == 'deterministic':\n",
    "            alpha = tensor.nnet.softmax(alpha.reshape([alpha_shp[0],alpha_shp[1]])) # softmax\n",
    "            ctx_ = (context * alpha[:,:,None]).sum(1) # current context\n",
    "            alpha_sample = alpha # you can return something else reasonable here to debug\n",
    "        else:\n",
    "            alpha = tensor.nnet.softmax(temperature_c*alpha.reshape([alpha_shp[0],alpha_shp[1]])) # softmax\n",
    "            # TODO return alpha_sample\n",
    "            if sampling:\n",
    "                alpha_sample = h_sampling_mask * trng.multinomial(pvals=alpha,dtype=theano.config.floatX)\\\n",
    "                               + (1.-h_sampling_mask) * alpha\n",
    "            else:\n",
    "                if argmax:\n",
    "                    alpha_sample = tensor.cast(tensor.eq(tensor.arange(alpha_shp[1])[None,:],\n",
    "                                               tensor.argmax(alpha,axis=1,keepdims=True)), theano.config.floatX)\n",
    "                else:\n",
    "                    alpha_sample = alpha\n",
    "            ctx_ = (context * alpha_sample[:,:,None]).sum(1) # current context\n",
    "\n",
    "        if options['selector']:\n",
    "            sel_ = tensor.nnet.sigmoid(tensor.dot(h_, tparams[_p(prefix, 'W_sel')])+tparams[_p(prefix,'b_sel')])\n",
    "            sel_ = sel_.reshape([sel_.shape[0]])\n",
    "            ctx_ = sel_[:,None] * ctx_\n",
    "\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')])\n",
    "        preact += x_\n",
    "        preact += tensor.dot(ctx_, tparams[_p(prefix, 'Wc')])\n",
    "\n",
    "        # Recover the activations to the lstm gates\n",
    "        # [equation (1)]\n",
    "        i = _slice(preact, 0, dim)\n",
    "        f = _slice(preact, 1, dim)\n",
    "        o = _slice(preact, 2, dim)\n",
    "        if options['use_dropout_lstm']:\n",
    "            i = i * _slice(dp_, 0, dim)\n",
    "            f = f * _slice(dp_, 1, dim)\n",
    "            o = o * _slice(dp_, 2, dim)\n",
    "        i = tensor.nnet.sigmoid(i)\n",
    "        f = tensor.nnet.sigmoid(f)\n",
    "        o = tensor.nnet.sigmoid(o)\n",
    "        c = tensor.tanh(_slice(preact, 3, dim))\n",
    "\n",
    "        # compute the new memory/hidden state\n",
    "        # if the mask is 0, just copy the previous state\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:,None] * c + (1. - m_)[:,None] * c_ \n",
    "\n",
    "        h = o * tensor.tanh(c)\n",
    "        h = m_[:,None] * h + (1. - m_)[:,None] * h_\n",
    "\n",
    "        rval = [h, c, alpha, alpha_sample, ctx_]\n",
    "        if options['selector']:\n",
    "            rval += [sel_]\n",
    "        rval += [pstate_, pctx_, i, f, o, preact, alpha_pre]+pctx_list\n",
    "        return rval\n",
    "\n",
    "    if options['use_dropout_lstm']:\n",
    "        if options['selector']:\n",
    "            _step0 = lambda m_, x_, dp_, h_, c_, a_, as_, ct_, sel_, pctx_: \\\n",
    "                            _step(m_, x_, h_, c_, a_, as_, ct_, pctx_, dp_)\n",
    "        else:\n",
    "            _step0 = lambda m_, x_, dp_, h_, c_, a_, as_, ct_, pctx_: \\\n",
    "                            _step(m_, x_, h_, c_, a_, as_, ct_, pctx_, dp_)\n",
    "        dp_shape = state_below.shape\n",
    "        if one_step:\n",
    "            dp_mask = tensor.switch(use_noise,\n",
    "                                    trng.binomial((dp_shape[0], 3*dim),\n",
    "                                                  p=0.5, n=1, dtype=state_below.dtype),\n",
    "                                    tensor.alloc(0.5, dp_shape[0], 3 * dim))\n",
    "        else:\n",
    "            dp_mask = tensor.switch(use_noise,\n",
    "                                    trng.binomial((dp_shape[0], dp_shape[1], 3*dim),\n",
    "                                                  p=0.5, n=1, dtype=state_below.dtype),\n",
    "                                    tensor.alloc(0.5, dp_shape[0], dp_shape[1], 3*dim))\n",
    "    else:\n",
    "        if options['selector']:\n",
    "            _step0 = lambda m_, x_, h_, c_, a_, as_, ct_, sel_, pctx_: _step(m_, x_, h_, c_, a_, as_, ct_, pctx_)\n",
    "        else:\n",
    "            _step0 = lambda m_, x_, h_, c_, a_, as_, ct_, pctx_: _step(m_, x_, h_, c_, a_, as_, ct_, pctx_)\n",
    "\n",
    "    if one_step:\n",
    "        if options['use_dropout_lstm']:\n",
    "            if options['selector']:\n",
    "                rval = _step0(mask, state_below, dp_mask, init_state, init_memory, None, None, None, None, pctx_)\n",
    "            else:\n",
    "                rval = _step0(mask, state_below, dp_mask, init_state, init_memory, None, None, None, pctx_)\n",
    "        else:\n",
    "            if options['selector']:\n",
    "                rval = _step0(mask, state_below, init_state, init_memory, None, None, None, None, pctx_)\n",
    "            else:\n",
    "                rval = _step0(mask, state_below, init_state, init_memory, None, None, None, pctx_)\n",
    "        return rval\n",
    "    else:\n",
    "        seqs = [mask, state_below]\n",
    "        if options['use_dropout_lstm']:\n",
    "            seqs += [dp_mask]\n",
    "        outputs_info = [init_state,\n",
    "                        init_memory,\n",
    "                        tensor.alloc(0., n_samples, pctx_.shape[1]),\n",
    "                        tensor.alloc(0., n_samples, pctx_.shape[1]),\n",
    "                        tensor.alloc(0., n_samples, context.shape[2])]\n",
    "        if options['selector']:\n",
    "            outputs_info += [tensor.alloc(0., n_samples)]\n",
    "        outputs_info += [None,\n",
    "                         None,\n",
    "                         None,\n",
    "                         None,\n",
    "                         None,\n",
    "                         None,\n",
    "                         None] + [None] # *options['n_layers_att']\n",
    "        rval, updates = theano.scan(_step0,\n",
    "                                    sequences=seqs,\n",
    "                                    outputs_info=outputs_info,\n",
    "                                    non_sequences=[pctx_],\n",
    "                                    name=_p(prefix, '_layers'),\n",
    "                                    n_steps=nsteps, profile=False)\n",
    "        return rval, updates\n",
    "    \n",
    "def validate_options(options):\n",
    "    # Put friendly reminders here\n",
    "    if options['dim_word'] > options['dim']:\n",
    "        warnings.warn('dim_word should only be as large as dim.')\n",
    "\n",
    "    if options['lstm_encoder']:\n",
    "        warnings.warn('Note that this is a 1-D bidirectional LSTM, not 2-D one.')\n",
    "\n",
    "    if options['use_dropout_lstm']:\n",
    "        warnings.warn('dropout in the lstm seems not to help')\n",
    "\n",
    "    # Other checks:\n",
    "    if options['attn_type'] not in ['stochastic', 'deterministic']:\n",
    "        raise ValueError(\"specified attention type is not correct\")\n",
    "\n",
    "    return options\n",
    "\n",
    "# some utilities\n",
    "def ortho_weight(ndim):\n",
    "    \"\"\"\n",
    "    Random orthogonal weights\n",
    "\n",
    "    Used by norm_weights(below), in which case, we\n",
    "    are ensuring that the rows are orthogonal\n",
    "    (i.e W = U \\Sigma V, U has the same\n",
    "    # of rows, V has the same # of cols)\n",
    "    \"\"\"\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, _, _ = numpy.linalg.svd(W)\n",
    "    return u.astype('float32')\n",
    "\n",
    "def norm_weight(nin,nout=None, scale=0.01, ortho=True):\n",
    "    \"\"\"\n",
    "    Random weights drawn from a Gaussian\n",
    "    \"\"\"\n",
    "    if nout is None:\n",
    "        nout = nin\n",
    "    if nout == nin and ortho:\n",
    "        W = ortho_weight(nin)\n",
    "    else:\n",
    "        W = scale * numpy.random.randn(nin, nout)\n",
    "    return W.astype('float32')\n",
    "\n",
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "    # embedding: [matrix E in paper]\n",
    "    params['Wemb'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "    ctx_dim = options['ctx_dim']\n",
    "    \n",
    "    # init_state, init_cell: [top right on page 4]\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_state', nin=ctx_dim, nout=options['dim'])\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_memory', nin=ctx_dim, nout=options['dim'])\n",
    "    # decoder: LSTM: [equation (1)/(2)/(3)]\n",
    "    params = get_layer('lstm_cond')[0](options, params, prefix='decoder',\n",
    "                                       nin=options['dim_word'], dim=options['dim'],\n",
    "                                       dimctx=ctx_dim)\n",
    "    \n",
    "    # readout: [equation (7)]\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_lstm', nin=options['dim'], nout=options['dim_word'])\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit', nin=options['dim_word'], nout=options['n_words'])\n",
    "\n",
    "    return params\n",
    "\n",
    "# build a training model\n",
    "def build_model(tparams, options, sampling=True):\n",
    "    \"\"\" Builds the entire computational graph used for training\n",
    "\n",
    "    [This function builds a model described in Section 3.1.2 onwards\n",
    "    as the convolutional feature are precomputed, some extra features\n",
    "    which were not used are also implemented here.]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tparams : OrderedDict\n",
    "        maps names of variables to theano shared variables\n",
    "    options : dict\n",
    "        big dictionary with all the settings and hyperparameters\n",
    "    sampling : boolean\n",
    "        [If it is true, when using stochastic attention, follows\n",
    "        the learning rule described in section 4. at the bottom left of\n",
    "        page 5]\n",
    "    Returns\n",
    "    -------\n",
    "    trng: theano random number generator\n",
    "        Used for dropout, stochastic attention, etc\n",
    "    use_noise: theano shared variable\n",
    "        flag that toggles noise on and off\n",
    "    [x, mask, ctx]: theano variables\n",
    "        Represent the captions, binary mask, and annotations\n",
    "        for a single batch (see dimensions below)\n",
    "    alphas: theano variables\n",
    "        Attention weights\n",
    "    alpha_sample: theano variable\n",
    "        Sampled attention weights used in REINFORCE for stochastic\n",
    "        attention: [see the learning rule in eq (12)]\n",
    "    cost: theano variable\n",
    "        negative log likelihood\n",
    "    opt_outs: OrderedDict\n",
    "        extra outputs required depending on configuration in options\n",
    "    \"\"\"\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy.float32(0.))\n",
    "\n",
    "    # description string: #words x #samples,\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    mask = tensor.matrix('mask', dtype='float32')\n",
    "    # context: #samples x #annotations x dim\n",
    "    ctx = tensor.tensor3('ctx', dtype='float32')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    # index into the word embedding matrix, shift it forward in time\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps, n_samples, options['dim_word']])\n",
    "    emb_shifted = tensor.zeros_like(emb)\n",
    "    emb_shifted = tensor.set_subtensor(emb_shifted[1:], emb[:-1])\n",
    "    emb = emb_shifted\n",
    "\n",
    "    ctx0 = ctx\n",
    "\n",
    "    # initial state/cell [top right on page 4]\n",
    "    ctx_mean = ctx0.mean(1)\n",
    "    #### FROM PAPER ######### C_0\n",
    "    init_state = get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_state', activ='tanh')\n",
    "    #### FROM PAPER ######### H_0\n",
    "    init_memory = get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_memory', activ='tanh')\n",
    "    # lstm decoder\n",
    "    # [equation (1), (2), (3) in section 3.1.2]\n",
    "    attn_updates = []\n",
    "    get_layer('lstm_cond')[1]\n",
    "    proj, updates = get_layer('lstm_cond')[1](tparams, emb, options,prefix='decoder',mask=mask, context=ctx0,one_step=False,init_state=init_state,init_memory=init_memory,trng=trng,use_noise=use_noise,sampling=sampling)\n",
    "    attn_updates += updates\n",
    "    proj_h = proj[0]\n",
    "\n",
    "    alphas = proj[2]\n",
    "    alpha_sample = proj[3]\n",
    "    ctxs = proj[4]\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        proj_h = dropout_layer(proj_h, use_noise, trng)\n",
    "\n",
    "    # compute word probabilities\n",
    "    # [equation (7)]\n",
    "    logit = get_layer('ff')[1](tparams, proj_h, options, prefix='ff_logit_lstm', activ='linear')\n",
    "    #if options['prev2out']:\n",
    "    #    logit += emb\n",
    "    #if options['ctx2out']:\n",
    "    #    logit += get_layer('ff')[1](tparams, ctxs, options, prefix='ff_logit_ctx', activ='linear')\n",
    "    logit = tanh(logit)\n",
    "    if options['use_dropout']:\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    # compute softmax\n",
    "    logit = get_layer('ff')[1](tparams, logit, options, prefix='ff_logit', activ='linear')\n",
    "    logit_shp = logit.shape\n",
    "    probs = tensor.nnet.softmax(logit.reshape([logit_shp[0]*logit_shp[1], logit_shp[2]]))\n",
    "\n",
    "    # Index into the computed probability to give the log likelihood\n",
    "    x_flat = x.flatten()\n",
    "    p_flat = probs.flatten()\n",
    "    cost = -tensor.log(p_flat[tensor.arange(x_flat.shape[0])*probs.shape[1]+x_flat]+1e-8)\n",
    "    cost = cost.reshape([x.shape[0], x.shape[1]])\n",
    "    masked_cost = cost * mask\n",
    "    cost = (masked_cost).sum(0)\n",
    "\n",
    "    # optional outputs\n",
    "    opt_outs = dict() \n",
    "\n",
    "    if options['attn_type'] == 'stochastic':\n",
    "        opt_outs['masked_cost'] = masked_cost # need this for reinforce later\n",
    "        opt_outs['attn_updates'] = attn_updates # this is to update the rng\n",
    "\n",
    "    return trng, use_noise, [x, mask, ctx], alphas, alpha_sample, cost, opt_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dim_word=100,  # word vector dimensionality\n",
    "          ctx_dim=512,  # context vector dimensionality\n",
    "          dim=1000,  # the number of LSTM units\n",
    "          attn_type='deterministic',  # [see section 4 from paper]\n",
    "          n_layers_att=1,  # number of layers used to compute the attention weights\n",
    "          n_layers_out=1,  # number of layers used to compute logit\n",
    "          n_layers_lstm=1,  # number of lstm layers - NOT USING DEEP DECODER\n",
    "          n_layers_init=1,  # number of layers to initialize LSTM at time 0\n",
    "          lstm_encoder=False,  # if True, run bidirectional LSTM on input units\n",
    "          prev2out=False,  # Feed previous word into logit\n",
    "          ctx2out=False,  # Feed attention weighted ctx into logit\n",
    "          alpha_entropy_c=0.002,  # hard attn param\n",
    "          RL_sumCost=True,  # hard attn param\n",
    "          semi_sampling_p=0.5,  # hard attn param\n",
    "          temperature=1.,  # hard attn param\n",
    "          patience=10,\n",
    "          max_epochs=5000,\n",
    "          dispFreq=100,\n",
    "          decay_c=0.,  # weight decay coeff\n",
    "          alpha_c=0.,  # doubly stochastic coeff\n",
    "          lrate=0.01,  # used only for SGD\n",
    "          selector=False,  # selector (see paper)\n",
    "          n_words=23112,  # vocab size for flickr30k\n",
    "          maxlen=100,  # maximum length of the description\n",
    "          optimizer='rmsprop',\n",
    "          batch_size = 16,\n",
    "          valid_batch_size = 16,\n",
    "          saveto='model.npz',  # relative path of saved model file\n",
    "          validFreq=1000,\n",
    "          saveFreq=1000,  # save the parameters after every saveFreq updates\n",
    "          sampleFreq=100,  # generate some samples after every sampleFreq updates\n",
    "          dataset='flickr30k',\n",
    "          dictionary=None,  # word dictionary\n",
    "          use_dropout=False,  # setting this true turns on dropout at various points\n",
    "          use_dropout_lstm=False,  # dropout on lstm gates\n",
    "          reload_=False,\n",
    "          save_per_epoch=False):\n",
    "    model_opts = locals().copy()\n",
    "    model_opts = validate_options(model_opts)\n",
    "    print model_opts\n",
    "    print 'Building model'\n",
    "    params = init_params(model_opts)\n",
    "    tparams = init_tparams(params)\n",
    "    # In order, we get:\n",
    "    #   1) trng - theano random number generator\n",
    "    #   2) use_noise - flag that turns on dropout\n",
    "    #   3) inps - inputs for f_grad_shared\n",
    "    #   4) cost - log likelihood for each sentence\n",
    "    #   5) opts_out - optional outputs (e.g selector)\n",
    "    trng, use_noise, inps, alphas, alphas_sample, cost, opt_outs = build_model(tparams, model_opts)\n",
    "    print 'Buliding sampler'\n",
    "    f_init, f_next = build_sampler(tparams, model_options, use_noise, trng)\n",
    "\n",
    "    # we want the cost without any the regularizers\n",
    "    f_log_probs = theano.function(inps, -cost, profile=False,\n",
    "                                        updates=opt_outs['attn_updates']\n",
    "                                        if model_options['attn_type']=='stochastic'\n",
    "                                        else None)\n",
    "\n",
    "    cost = cost.mean()\n",
    "    # add L2 regularization costs\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy.float32(decay_c), name='decay_c')\n",
    "        weight_decay = 0.\n",
    "        for kk, vv in tparams.iteritems():\n",
    "            weight_decay += (vv ** 2).sum()\n",
    "        weight_decay *= decay_c\n",
    "        cost += weight_decay\n",
    "\n",
    "    # Doubly stochastic regularization\n",
    "    if alpha_c > 0.:\n",
    "        alpha_c = theano.shared(numpy.float32(alpha_c), name='alpha_c')\n",
    "        alpha_reg = alpha_c * ((1.-alphas.sum(0))**2).sum(0).mean()\n",
    "        cost += alpha_reg\n",
    "\n",
    "    hard_attn_updates = []\n",
    "    # Backprop!\n",
    "    if model_options['attn_type'] == 'deterministic':\n",
    "        grads = tensor.grad(cost, wrt=itemlist(tparams))\n",
    "    else:\n",
    "        # shared variables for hard attention\n",
    "        baseline_time = theano.shared(numpy.float32(0.), name='baseline_time')\n",
    "        opt_outs['baseline_time'] = baseline_time\n",
    "        alpha_entropy_c = theano.shared(numpy.float32(alpha_entropy_c), name='alpha_entropy_c')\n",
    "        alpha_entropy_reg = alpha_entropy_c * (alphas*tensor.log(alphas)).mean()\n",
    "        # [see Section 4.1: Stochastic \"Hard\" Attention for derivation of this learning rule]\n",
    "        if model_options['RL_sumCost']:\n",
    "            grads = tensor.grad(cost, wrt=itemlist(tparams),\n",
    "                                disconnected_inputs='raise',\n",
    "                                known_grads={alphas:(baseline_time-opt_outs['masked_cost'].mean(0))[None,:,None]/10.*\n",
    "                                            (-alphas_sample/alphas) + alpha_entropy_c*(tensor.log(alphas) + 1)})\n",
    "        else:\n",
    "            grads = tensor.grad(cost, wrt=itemlist(tparams),\n",
    "                            disconnected_inputs='raise',\n",
    "                            known_grads={alphas:opt_outs['masked_cost'][:,:,None]/10.*\n",
    "                            (alphas_sample/alphas) + alpha_entropy_c*(tensor.log(alphas) + 1)})\n",
    "        # [equation on bottom left of page 5]\n",
    "        hard_attn_updates += [(baseline_time, baseline_time * 0.9 + 0.1 * opt_outs['masked_cost'].mean())]\n",
    "        # updates from scan\n",
    "        hard_attn_updates += opt_outs['attn_updates']\n",
    "\n",
    "    # to getthe cost after regularization or the gradients, use this\n",
    "    # f_cost = theano.function([x, mask, ctx], cost, profile=False)\n",
    "    # f_grad = theano.function([x, mask, ctx], grads, profile=False)\n",
    "\n",
    "    # f_grad_shared computes the cost and updates adaptive learning rate variables\n",
    "    # f_update updates the weights of the model\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    f_grad_shared, f_update = eval(optimizer)(lr, tparams, grads, inps, cost, hard_attn_updates)\n",
    "\n",
    "    print 'Optimization'\n",
    "\n",
    "    # [See note in section 4.3 of paper]\n",
    "    train_iter = HomogeneousData(train, batch_size=batch_size, maxlen=maxlen)\n",
    "\n",
    "    if valid:\n",
    "        kf_valid = KFold(len(valid[0]), n_folds=len(valid[0])/valid_batch_size, shuffle=False)\n",
    "    if test:\n",
    "        kf_test = KFold(len(test[0]), n_folds=len(test[0])/valid_batch_size, shuffle=False)\n",
    "\n",
    "    # history_errs is a bare-bones training log that holds the validation and test error\n",
    "    history_errs = []\n",
    "    # reload history\n",
    "    if reload_ and os.path.exists(saveto):\n",
    "        history_errs = numpy.load(saveto)['history_errs'].tolist()\n",
    "    best_p = None\n",
    "    bad_counter = 0\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = len(train[0])/batch_size\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = len(train[0])/batch_size\n",
    "    if sampleFreq == -1:\n",
    "        sampleFreq = len(train[0])/batch_size\n",
    "\n",
    "    uidx = 0\n",
    "    estop = False\n",
    "    for eidx in xrange(max_epochs):\n",
    "        n_samples = 0\n",
    "\n",
    "        print 'Epoch ', eidx\n",
    "\n",
    "        for caps in train_iter:\n",
    "            n_samples += len(caps)\n",
    "            uidx += 1\n",
    "            # turn on dropout\n",
    "            use_noise.set_value(1.)\n",
    "\n",
    "            # preprocess the caption, recording the\n",
    "            # time spent to help detect bottlenecks\n",
    "            pd_start = time.time()\n",
    "            x, mask, ctx = prepare_data(caps,\n",
    "                                        train[1],\n",
    "                                        worddict,\n",
    "                                        maxlen=maxlen,\n",
    "                                        n_words=n_words)\n",
    "            pd_duration = time.time() - pd_start\n",
    "\n",
    "            if x is None:\n",
    "                print 'Minibatch with zero sample under length ', maxlen\n",
    "                continue\n",
    "\n",
    "            # get the cost for the minibatch, and update the weights\n",
    "            ud_start = time.time()\n",
    "            cost = f_grad_shared(x, mask, ctx)\n",
    "            f_update(lrate)\n",
    "            ud_duration = time.time() - ud_start # some monitoring for each mini-batch\n",
    "\n",
    "            # Numerical stability check\n",
    "            if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                print 'NaN detected'\n",
    "                return 1., 1., 1.\n",
    "\n",
    "            if numpy.mod(uidx, dispFreq) == 0:\n",
    "                print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost, 'PD ', pd_duration, 'UD ', ud_duration\n",
    "\n",
    "            # Checkpoint\n",
    "            if numpy.mod(uidx, saveFreq) == 0:\n",
    "                print 'Saving...',\n",
    "\n",
    "                if best_p is not None:\n",
    "                    params = copy.copy(best_p)\n",
    "                else:\n",
    "                    params = unzip(tparams)\n",
    "                numpy.savez(saveto, history_errs=history_errs, **params)\n",
    "                pkl.dump(model_options, open('%s.pkl'%saveto, 'wb'))\n",
    "                print 'Done'\n",
    "\n",
    "            # Print a generated sample as a sanity check\n",
    "            if numpy.mod(uidx, sampleFreq) == 0:\n",
    "                # turn off dropout first\n",
    "                use_noise.set_value(0.)\n",
    "                x_s = x\n",
    "                mask_s = mask\n",
    "                ctx_s = ctx\n",
    "                # generate and decode the a subset of the current training batch\n",
    "                for jj in xrange(numpy.minimum(10, len(caps))):\n",
    "                    sample, score = gen_sample(tparams, f_init, f_next, ctx_s[jj], model_options,\n",
    "                                               trng=trng, k=5, maxlen=30, stochastic=False)\n",
    "                    # Decode the sample from encoding back to words\n",
    "                    print 'Truth ',jj,': ',\n",
    "                    for vv in x_s[:,jj]:\n",
    "                        if vv == 0:\n",
    "                            break\n",
    "                        if vv in word_idict:\n",
    "                            print word_idict[vv],\n",
    "                        else:\n",
    "                            print 'UNK',\n",
    "                    print\n",
    "                    for kk, ss in enumerate([sample[0]]):\n",
    "                        print 'Sample (', kk,') ', jj, ': ',\n",
    "                        for vv in ss:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in word_idict:\n",
    "                                print word_idict[vv],\n",
    "                            else:\n",
    "                                print 'UNK',\n",
    "                    print\n",
    "\n",
    "            # Log validation loss + checkpoint the model with the best validation log likelihood\n",
    "            if numpy.mod(uidx, validFreq) == 0:\n",
    "                use_noise.set_value(0.)\n",
    "                train_err = 0\n",
    "                valid_err = 0\n",
    "                test_err = 0\n",
    "\n",
    "                if valid:\n",
    "                    valid_err = -pred_probs(f_log_probs, model_options, worddict, prepare_data, valid, kf_valid).mean()\n",
    "                if test:\n",
    "                    test_err = -pred_probs(f_log_probs, model_options, worddict, prepare_data, test, kf_test).mean()\n",
    "\n",
    "                history_errs.append([valid_err, test_err])\n",
    "\n",
    "                # the model with the best validation long likelihood is saved seperately with a different name\n",
    "                if uidx == 0 or valid_err <= numpy.array(history_errs)[:,0].min():\n",
    "                    best_p = unzip(tparams)\n",
    "                    print 'Saving model with best validation ll'\n",
    "                    params = copy.copy(best_p)\n",
    "                    params = unzip(tparams)\n",
    "                    numpy.savez(saveto+'_bestll', history_errs=history_errs, **params)\n",
    "                    bad_counter = 0\n",
    "\n",
    "                # abort training if perplexity has been increasing for too long\n",
    "                if eidx > patience and len(history_errs) > patience and valid_err >= numpy.array(history_errs)[:-patience,0].min():\n",
    "                    bad_counter += 1\n",
    "                    if bad_counter > patience:\n",
    "                        print 'Early Stop!'\n",
    "                        estop = True\n",
    "                        break\n",
    "\n",
    "                print 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err\n",
    "\n",
    "        print 'Seen %d samples' % n_samples\n",
    "\n",
    "        if estop:\n",
    "            break\n",
    "\n",
    "        if save_per_epoch:\n",
    "            numpy.savez(saveto + '_epoch_' + str(eidx + 1), history_errs=history_errs, **unzip(tparams))\n",
    "\n",
    "    # use the best nll parameters for final checkpoint (if they exist)\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    train_err = 0\n",
    "    valid_err = 0\n",
    "    test_err = 0\n",
    "    if valid:\n",
    "        valid_err = -pred_probs(f_log_probs, model_options, worddict, prepare_data, valid, kf_valid)\n",
    "    if test:\n",
    "        test_err = -pred_probs(f_log_probs, model_options, worddict, prepare_data, test, kf_test)\n",
    "\n",
    "    print 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err\n",
    "\n",
    "    params = copy.copy(best_p)\n",
    "    numpy.savez(saveto, zipped_params=best_p, train_err=train_err,\n",
    "                valid_err=valid_err, test_err=test_err, history_errs=history_errs,\n",
    "                **params)\n",
    "\n",
    "    return train_err, valid_err, test_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lrate': 0.01, 'decay_c': 0.0, 'patience': 10, 'save_per_epoch': False, 'n_layers_init': 1, 'RL_sumCost': True, 'max_epochs': 5000, 'dispFreq': 100, 'attn_type': 'deterministic', 'alpha_c': 0.0, 'temperature': 1.0, 'n_layers_att': 1, 'saveto': 'model.npz', 'ctx_dim': 512, 'valid_batch_size': 16, 'lstm_encoder': False, 'n_layers_lstm': 1, 'optimizer': 'rmsprop', 'validFreq': 1000, 'dictionary': None, 'batch_size': 16, 'selector': False, 'n_words': 23112, 'dataset': 'flickr30k', 'use_dropout_lstm': False, 'prev2out': False, 'dim': 1000, 'use_dropout': False, 'dim_word': 100, 'sampleFreq': 100, 'semi_sampling_p': 0.5, 'n_layers_out': 1, 'saveFreq': 1000, 'maxlen': 100, 'alpha_entropy_c': 0.002, 'ctx2out': False, 'reload_': False}\n",
      "Building model\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a sampler\n",
    "def build_sampler(tparams, options, use_noise, trng, sampling=True):\n",
    "    \"\"\" Builds a sampler used for generating from the model\n",
    "    Parameters\n",
    "    ----------\n",
    "        See build_model function above\n",
    "    Returns\n",
    "    -------\n",
    "    f_init : theano function\n",
    "        Input: annotation, Output: initial lstm state and memory\n",
    "        (also performs transformation on ctx0 if using lstm_encoder)\n",
    "    f_next: theano function\n",
    "        Takes the previous word/state/memory + ctx0 and runs ne\n",
    "        step through the lstm (used for beam search)\n",
    "    \"\"\"\n",
    "    # context: #annotations x dim\n",
    "    ctx = tensor.matrix('ctx_sampler', dtype='float32')\n",
    "    if options['lstm_encoder']:\n",
    "        # encoder\n",
    "        ctx_fwd = get_layer('lstm')[1](tparams, ctx,\n",
    "                                       options, prefix='encoder')[0]\n",
    "        ctx_rev = get_layer('lstm')[1](tparams, ctx[::-1,:],\n",
    "                                       options, prefix='encoder_rev')[0][::-1,:]\n",
    "        ctx = tensor.concatenate((ctx_fwd, ctx_rev), axis=1)\n",
    "\n",
    "    # initial state/cell\n",
    "    ctx_mean = ctx.mean(0)\n",
    "    for lidx in xrange(1, options['n_layers_init']):\n",
    "        ctx_mean = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                      prefix='ff_init_%d'%lidx, activ='rectifier')\n",
    "        if options['use_dropout']:\n",
    "            ctx_mean = dropout_layer(ctx_mean, use_noise, trng)\n",
    "    init_state = [get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_state', activ='tanh')]\n",
    "    init_memory = [get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_memory', activ='tanh')]\n",
    "    if options['n_layers_lstm'] > 1:\n",
    "        for lidx in xrange(1, options['n_layers_lstm']):\n",
    "            init_state.append(get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_state_%d'%lidx, activ='tanh'))\n",
    "            init_memory.append(get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_memory_%d'%lidx, activ='tanh'))\n",
    "\n",
    "    print 'Building f_init...',\n",
    "    \n",
    "    f_init = theano.function([ctx], [ctx]+init_state+init_memory, name='f_init', profile=False)\n",
    "    print 'Done'\n",
    "\n",
    "    # build f_next\n",
    "    ctx = tensor.matrix('ctx_sampler', dtype='float32')\n",
    "    x = tensor.vector('x_sampler', dtype='int64')\n",
    "    init_state = [tensor.matrix('init_state', dtype='float32')]\n",
    "    init_memory = [tensor.matrix('init_memory', dtype='float32')]\n",
    "    if options['n_layers_lstm'] > 1:\n",
    "        for lidx in xrange(1, options['n_layers_lstm']):\n",
    "            init_state.append(tensor.matrix('init_state', dtype='float32'))\n",
    "            init_memory.append(tensor.matrix('init_memory', dtype='float32'))\n",
    "\n",
    "    # for the first word (which is coded with -1), emb should be all zero\n",
    "    emb = tensor.switch(x[:,None] < 0, tensor.alloc(0., 1, tparams['Wemb'].shape[1]),\n",
    "                        tparams['Wemb'][x])\n",
    "\n",
    "    proj = get_layer('lstm_cond')[1](tparams, emb, options,\n",
    "                                     prefix='decoder',\n",
    "                                     mask=None, context=ctx,\n",
    "                                     one_step=True,\n",
    "                                     init_state=init_state[0],\n",
    "                                     init_memory=init_memory[0],\n",
    "                                     trng=trng,\n",
    "                                     use_noise=use_noise,\n",
    "                                     sampling=sampling)\n",
    "\n",
    "    next_state, next_memory, ctxs = [proj[0]], [proj[1]], [proj[4]]\n",
    "    proj_h = proj[0]\n",
    "    if options['n_layers_lstm'] > 1:\n",
    "        for lidx in xrange(1, options['n_layers_lstm']):\n",
    "            proj = get_layer('lstm_cond')[1](tparams, proj_h, options,\n",
    "                                             prefix='decoder_%d'%lidx,\n",
    "                                             context=ctx,\n",
    "                                             one_step=True,\n",
    "                                             init_state=init_state[lidx],\n",
    "                                             init_memory=init_memory[lidx],\n",
    "                                             trng=trng,\n",
    "                                             use_noise=use_noise,\n",
    "                                             sampling=sampling)\n",
    "            next_state.append(proj[0])\n",
    "            next_memory.append(proj[1])\n",
    "            ctxs.append(proj[4])\n",
    "            proj_h = proj[0]\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        proj_h = dropout_layer(proj[0], use_noise, trng)\n",
    "    else:\n",
    "        proj_h = proj[0]\n",
    "    logit = get_layer('ff')[1](tparams, proj_h, options, prefix='ff_logit_lstm', activ='linear')\n",
    "    if options['prev2out']:\n",
    "        logit += emb\n",
    "    if options['ctx2out']:\n",
    "        logit += get_layer('ff')[1](tparams, ctxs[-1], options, prefix='ff_logit_ctx', activ='linear')\n",
    "    logit = tanh(logit)\n",
    "    if options['use_dropout']:\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "    if options['n_layers_out'] > 1:\n",
    "        for lidx in xrange(1, options['n_layers_out']):\n",
    "            logit = get_layer('ff')[1](tparams, logit, options, prefix='ff_logit_h%d'%lidx, activ='rectifier')\n",
    "            if options['use_dropout']:\n",
    "                logit = dropout_layer(logit, use_noise, trng)\n",
    "    logit = get_layer('ff')[1](tparams, logit, options, prefix='ff_logit', activ='linear')\n",
    "    logit_shp = logit.shape\n",
    "    next_probs = tensor.nnet.softmax(logit)\n",
    "    next_sample = trng.multinomial(pvals=next_probs).argmax(1)\n",
    "\n",
    "    # next word probability\n",
    "    f_next = theano.function([x, ctx]+init_state+init_memory, [next_probs, next_sample]+next_state+next_memory, name='f_next', profile=False)\n",
    "\n",
    "    return f_init, f_next\n",
    "\n",
    "# generate sample\n",
    "def gen_sample(tparams, f_init, f_next, ctx0, options,\n",
    "               trng=None, k=1, maxlen=30, stochastic=False):\n",
    "    \"\"\"Generate captions with beam search.\n",
    "    \n",
    "    This function uses the beam search algorithm to conditionally\n",
    "    generate candidate captions. Supports beamsearch and stochastic\n",
    "    sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tparams : OrderedDict()\n",
    "        dictionary of theano shared variables represented weight\n",
    "        matricies\n",
    "    f_init : theano function\n",
    "        input: annotation, output: initial lstm state and memory \n",
    "        (also performs transformation on ctx0 if using lstm_encoder)\n",
    "    f_next: theano function\n",
    "        takes the previous word/state/memory + ctx0 and runs one\n",
    "        step through the lstm\n",
    "    ctx0 : numpy array\n",
    "        annotation from convnet, of dimension #annotations x # dimension\n",
    "        [e.g (196 x 512)]\n",
    "    options : dict\n",
    "        dictionary of flags and options\n",
    "    trng : random number generator\n",
    "    k : int\n",
    "        size of beam search\n",
    "    maxlen : int\n",
    "        maximum allowed caption size\n",
    "    stochastic : bool\n",
    "        if True, sample stochastically \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample : list of list\n",
    "        each sublist contains an (encoded) sample from the model \n",
    "    sample_score : numpy array\n",
    "        scores of each sample\n",
    "    \"\"\"\n",
    "    if k > 1:\n",
    "        assert not stochastic, 'Beam search does not support stochastic sampling'\n",
    "\n",
    "    sample = []\n",
    "    sample_score = []\n",
    "    if stochastic:\n",
    "        sample_score = 0\n",
    "\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "\n",
    "    hyp_samples = [[]] * live_k\n",
    "    hyp_scores = numpy.zeros(live_k).astype('float32')\n",
    "    hyp_states = []\n",
    "    hyp_memories = []\n",
    "\n",
    "    # only matters if we use lstm encoder\n",
    "    rval = f_init(ctx0)\n",
    "    ctx0 = rval[0]\n",
    "    next_state = []\n",
    "    next_memory = []\n",
    "    # the states are returned as a: (dim,) and this is just a reshape to (1, dim)\n",
    "    for lidx in xrange(options['n_layers_lstm']):\n",
    "        next_state.append(rval[1+lidx])\n",
    "        next_state[-1] = next_state[-1].reshape([1, next_state[-1].shape[0]])\n",
    "    for lidx in xrange(options['n_layers_lstm']):\n",
    "        next_memory.append(rval[1+options['n_layers_lstm']+lidx])\n",
    "        next_memory[-1] = next_memory[-1].reshape([1, next_memory[-1].shape[0]])\n",
    "    # reminder: if next_w = -1, the switch statement\n",
    "    # in build_sampler is triggered -> (empty word embeddings)  \n",
    "    next_w = -1 * numpy.ones((1,)).astype('int64')\n",
    "\n",
    "    for ii in xrange(maxlen):\n",
    "        # our \"next\" state/memory in our previous step is now our \"initial\" state and memory\n",
    "        rval = f_next(*([next_w, ctx0]+next_state+next_memory))\n",
    "        next_p = rval[0]\n",
    "        next_w = rval[1]\n",
    "\n",
    "        # extract all the states and memories\n",
    "        next_state = []\n",
    "        next_memory = []\n",
    "        for lidx in xrange(options['n_layers_lstm']):\n",
    "            next_state.append(rval[2+lidx])\n",
    "            next_memory.append(rval[2+options['n_layers_lstm']+lidx])\n",
    "\n",
    "        if stochastic:\n",
    "            sample.append(next_w[0]) # if we are using stochastic sampling this easy\n",
    "            sample_score += next_p[0,next_w[0]]\n",
    "            if next_w[0] == 0:\n",
    "                break\n",
    "        else:\n",
    "            cand_scores = hyp_scores[:,None] - numpy.log(next_p) \n",
    "            cand_flat = cand_scores.flatten()\n",
    "            ranks_flat = cand_flat.argsort()[:(k-dead_k)] # (k-dead_k) numpy array of with min nll\n",
    "\n",
    "            voc_size = next_p.shape[1]\n",
    "            # indexing into the correct selected captions\n",
    "            trans_indices = ranks_flat / voc_size\n",
    "            word_indices = ranks_flat % voc_size\n",
    "            costs = cand_flat[ranks_flat] # extract costs from top hypothesis\n",
    "\n",
    "            # a bunch of lists to hold future hypothesis\n",
    "            new_hyp_samples = []\n",
    "            new_hyp_scores = numpy.zeros(k-dead_k).astype('float32')\n",
    "            new_hyp_states = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                new_hyp_states.append([])\n",
    "            new_hyp_memories = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                new_hyp_memories.append([])\n",
    "\n",
    "            # get the corresponding hypothesis and append the predicted word\n",
    "            for idx, [ti, wi] in enumerate(zip(trans_indices, word_indices)):\n",
    "                new_hyp_samples.append(hyp_samples[ti]+[wi])\n",
    "                new_hyp_scores[idx] = copy.copy(costs[idx]) # copy in the cost of that hypothesis \n",
    "                for lidx in xrange(options['n_layers_lstm']):\n",
    "                    new_hyp_states[lidx].append(copy.copy(next_state[lidx][ti]))\n",
    "                for lidx in xrange(options['n_layers_lstm']):\n",
    "                    new_hyp_memories[lidx].append(copy.copy(next_memory[lidx][ti]))\n",
    "\n",
    "            # check the finished samples for <eos> character\n",
    "            new_live_k = 0\n",
    "            hyp_samples = []\n",
    "            hyp_scores = []\n",
    "            hyp_states = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                hyp_states.append([])\n",
    "            hyp_memories = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                hyp_memories.append([])\n",
    "\n",
    "            for idx in xrange(len(new_hyp_samples)):\n",
    "                if new_hyp_samples[idx][-1] == 0:\n",
    "                    sample.append(new_hyp_samples[idx])\n",
    "                    sample_score.append(new_hyp_scores[idx])\n",
    "                    dead_k += 1 # completed sample!\n",
    "                else:\n",
    "                    new_live_k += 1 # collect collect correct states/memories\n",
    "                    hyp_samples.append(new_hyp_samples[idx])\n",
    "                    hyp_scores.append(new_hyp_scores[idx])\n",
    "                    for lidx in xrange(options['n_layers_lstm']):\n",
    "                        hyp_states[lidx].append(new_hyp_states[lidx][idx])\n",
    "                    for lidx in xrange(options['n_layers_lstm']):\n",
    "                        hyp_memories[lidx].append(new_hyp_memories[lidx][idx])\n",
    "            hyp_scores = numpy.array(hyp_scores)\n",
    "            live_k = new_live_k\n",
    "\n",
    "            if new_live_k < 1:\n",
    "                break\n",
    "            if dead_k >= k:\n",
    "                break\n",
    "\n",
    "            next_w = numpy.array([w[-1] for w in hyp_samples])\n",
    "            next_state = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                next_state.append(numpy.array(hyp_states[lidx]))\n",
    "            next_memory = []\n",
    "            for lidx in xrange(options['n_layers_lstm']):\n",
    "                next_memory.append(numpy.array(hyp_memories[lidx]))\n",
    "\n",
    "    if not stochastic:\n",
    "        # dump every remaining one\n",
    "        if live_k > 0:\n",
    "            for idx in xrange(live_k):\n",
    "                sample.append(hyp_samples[idx])\n",
    "                sample_score.append(hyp_scores[idx])\n",
    "\n",
    "    return sample, sample_score\n",
    "\n",
    "\n",
    "def pred_probs(f_log_probs, options, worddict, prepare_data, data, iterator, verbose=False):\n",
    "    \"\"\" Get log probabilities of captions\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_log_probs : theano function\n",
    "        compute the log probability of a x given the context\n",
    "    options : dict\n",
    "        options dictionary\n",
    "    worddict : dict\n",
    "        maps words to one-hot encodings\n",
    "    prepare_data : function\n",
    "        see corresponding dataset class for details\n",
    "    data : numpy array\n",
    "        output of load_data, see corresponding dataset class\n",
    "    iterator : KFold\n",
    "        indices from scikit-learn KFold\n",
    "    verbose : boolean\n",
    "        if True print progress\n",
    "    Returns\n",
    "    -------\n",
    "    probs : numpy array\n",
    "        array of log probabilities indexed by example\n",
    "    \"\"\"\n",
    "    n_samples = len(data[0])\n",
    "    probs = numpy.zeros((n_samples, 1)).astype('float32')\n",
    "\n",
    "    n_done = 0\n",
    "\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, ctx = prepare_data([data[0][t] for t in valid_index],\n",
    "                                     data[1],\n",
    "                                     worddict,\n",
    "                                     maxlen=None,\n",
    "                                     n_words=options['n_words'])\n",
    "        pred_probs = f_log_probs(x,mask,ctx)\n",
    "        probs[valid_index] = pred_probs[:,None]\n",
    "\n",
    "        n_done += len(valid_index)\n",
    "        if verbose:\n",
    "            print '%d/%d samples computed'%(n_done,n_samples)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# push parameters to Theano shared variables\n",
    "def zipp(params, tparams):\n",
    "    for kk, vv in params.iteritems():\n",
    "        tparams[kk].set_value(vv)\n",
    "\n",
    "# pull parameters from Theano shared variables\n",
    "def unzip(zipped):\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.iteritems():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params\n",
    "\n",
    "# get the list of parameters: Note that tparams must be OrderedDict\n",
    "def itemlist(tparams):\n",
    "    return [vv for kk, vv in tparams.iteritems()]\n",
    "\n",
    "# dropout in theano\n",
    "def dropout_layer(state_before, use_noise, trng):\n",
    "    \"\"\"\n",
    "    tensor switch is like an if statement that checks the\n",
    "    value of the theano shared variable (use_noise), before\n",
    "    either dropping out the state_before tensor or\n",
    "    computing the appropriate activation. During training/testing\n",
    "    use_noise is toggled on and off.\n",
    "    \"\"\"\n",
    "    proj = tensor.switch(use_noise,\n",
    "                         state_before *\n",
    "                         trng.binomial(state_before.shape, p=0.5, n=1, dtype=state_before.dtype),\n",
    "                         state_before * 0.5)\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
