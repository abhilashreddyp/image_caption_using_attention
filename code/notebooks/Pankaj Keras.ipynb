{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "import six.moves.cPickle as pkl\n",
    "\n",
    "\n",
    "embedding_vector_length=256\n",
    "max_caption_len=16\n",
    "output_dim=4096\n",
    "\n",
    "image_dir=\"images/\"\n",
    "captions_dir=\"captions/\"\n",
    "vocab_dir=\"vocab/\"\n",
    "weights_dir=\"weights/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    images = OrderedDict()\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        image = load_image(image_file)\n",
    "        images[image_file] = image\n",
    "    return images\n",
    "\n",
    "def load_image(image_path):\n",
    "    im = cv2.resize(cv2.imread(os.path.join(image_dir,image_path)), (224, 224)).astype(np.float32)\n",
    "    im[:,:,0] -= 103.939\n",
    "    im[:,:,1] -= 116.779\n",
    "    im[:,:,2] -= 123.68\n",
    "    im = im.transpose((2,0,1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    return im\n",
    "\n",
    "def load_captions():\n",
    "    captions = OrderedDict()\n",
    "    lines = []\n",
    "    with open(os.path.join(captions_dir, \"ref.txt\")) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        space_index = line.index(\" \")\n",
    "        image_name = line[0:space_index].strip()\n",
    "        caption = line[space_index+1:].strip()\n",
    "        captions[image_name] = caption\n",
    "    return captions    \n",
    "\n",
    "def load_vocabulary():\n",
    "    with open(os.path.join(vocab_dir,'dictionary.pkl'), 'rb') as f:\n",
    "        worddict = pkl.load(f)\n",
    "    vocab = defaultdict(lambda : 1) # return 1, the index for 'UNK' by default\n",
    "    vocab2 = defaultdict(lambda : \"\")\n",
    "    for word, index in worddict.items():\n",
    "        vocab[word] = index\n",
    "        vocab2[index] = word\n",
    "    vocab['<eos>'] = 0\n",
    "    vocab['UNK'] = 1\n",
    "    vocab['.'] = len(vocab)\n",
    "    vocab2[0] = '<eos>'\n",
    "    vocab2[1] = 'UNK'\n",
    "    vocab[len(vocab)] = '.'\n",
    "    #print(vocab)\n",
    "    #print(len(vocab))\n",
    "    #vocab_size = len(vocab.keys())\n",
    "    return vocab, vocab2\n",
    "\n",
    "def gen_image_partial_captions(images, captions, vocab):\n",
    "    a_images = []\n",
    "    a_captions = []\n",
    "    next_words = []\n",
    "    #vocab_size = len(vocab)\n",
    "    for image_name in images.keys():\n",
    "        caption = captions[image_name]\n",
    "        words = [\"\"]\n",
    "        words.extend(caption.split(\" \"))\n",
    "        words.append('<eos>')\n",
    "        #print(words)\n",
    "        partial_caption_ar = np.zeros(max_caption_len, dtype=np.int)\n",
    "        #No need to process <eos> tag\n",
    "        for i in range(len(words) - 1):\n",
    "            pc_copy = partial_caption_ar.copy()\n",
    "            word = words[i]\n",
    "            #print(word)\n",
    "            index = -1 if i == 0 else vocab[word]\n",
    "            if index == 1:\n",
    "                print(\" wtf index 1 for word %s\"%word)\n",
    "            pc_copy[i] = index\n",
    "            #Generate input image and partial caption vectors\n",
    "            a_images.append(images[image_name])\n",
    "            a_captions.append(pc_copy)\n",
    "            #Generate next word output vector\n",
    "            next_word = words[i + 1]\n",
    "            next_word_index = vocab[next_word]\n",
    "            if next_word_index == 1:\n",
    "                print(\"wtf next_word_index 1 for next_word %s\"%next_word)\n",
    "            #print(next_word_index)\n",
    "            next_word_ar = np.zeros(vocab_size, dtype=np.int)\n",
    "            next_word_ar[next_word_index] = 1\n",
    "            next_words.append(next_word_ar)\n",
    "            #print(next_word_ar.shape)\n",
    "    #print(next_words)\n",
    "    v_i = np.vstack(a_images)\n",
    "    v_c = np.vstack(a_captions)\n",
    "    v_nw = np.vstack(next_words)\n",
    "    return v_i, v_c, v_nw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Merge\n",
    "from keras.layers.core import Flatten, Dense, Dropout, RepeatVector\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "import six.moves.cPickle as pkl\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "#vocab_size=1000\n",
    "#embedding_vector_length=256\n",
    "#max_caption_len=16\n",
    "#output_dim=1000\n",
    "\n",
    "#image_dir=\"images/\"\n",
    "#captions_dir=\"captions/\"\n",
    "#vocab_dir=\"vocab/\"\n",
    "#weights_dir=\"weights/\"\n",
    "\n",
    "def language_model():\n",
    "    model = Sequential()\n",
    "    print('Adding Embedding')\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_caption_len))\n",
    "    print('Adding LSTM')\n",
    "    model.add(LSTM(output_dim, return_sequences=True))\n",
    "    print('Adding TimeDistributed Dense')\n",
    "    model.add(TimeDistributed(Dense(output_dim)))\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def pop(model):\n",
    "    '''Removes a layer instance on top of the layer stack.\n",
    "    This code is thanks to @joelthchao https://github.com/fchollet/keras/issues/2371#issuecomment-211734276\n",
    "    '''\n",
    "    if not model.outputs:\n",
    "        raise Exception('Sequential model cannot be popped: model is empty.')\n",
    "    else:\n",
    "        model.layers.pop()\n",
    "        if not model.layers:\n",
    "            model.outputs = []\n",
    "            model.inbound_nodes = []\n",
    "            model.outbound_nodes = []\n",
    "        else:\n",
    "            model.layers[-1].outbound_nodes = []\n",
    "            model.outputs = [model.layers[-1].output]\n",
    "        model.built = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def VGG_16(weights_file=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    #print(model.summary())\n",
    "    print('Loading weights')\n",
    "    if weights_file:\n",
    "        model = load_weights(model, weights_file)\n",
    "    print('Loaded weights')\n",
    "    #model = pop(model)\n",
    "    #model = pop(model)\n",
    "    #model.layers.pop()\n",
    "    #model.layers.pop()\n",
    "    #model.layers.pop()\n",
    "\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def build_model(weights_path):\n",
    "    image_model = VGG_16(weights_path)\n",
    "    image_model.add(RepeatVector(max_caption_len))\n",
    "    print('Built Image Model')\n",
    "    print('Building Language Model')\n",
    "    lang_model = language_model()\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, lang_model], mode='concat',  concat_axis=-1))\n",
    "    model.add(LSTM(embedding_vector_length, return_sequences=False))\n",
    "    #print(vocab_size)\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(images, partial_captions, next_words, v_size):\n",
    "    global vocab_size\n",
    "    vocab_size = v_size\n",
    "    model=build_model('vgg16_weights.h5')\n",
    "    print('Built model.')\n",
    "    print('Compiling Now')\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print('Fitting Now')\n",
    "    #print(images.shape)\n",
    "    #print(partial_captions.shape)\n",
    "    #print(next_words.shape)\n",
    "    model.fit([images, partial_captions], next_words, batch_size=20, nb_epoch=10)\n",
    "    return model\n",
    "\n",
    "def load_weights(model, weights_file):\n",
    "    f = h5py.File(os.path.join(weights_dir, weights_file))\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    return model\n",
    "\n",
    "def predict(model, d_images, index_to_word):\n",
    "    for image in d_images.values():\n",
    "        caption = np.zeros(max_caption_len).reshape(1, 16)\n",
    "        print(caption.shape)\n",
    "        caption[0] = -1\n",
    "        count=0\n",
    "        sentence = []\n",
    "        while True:\n",
    "            out = model.predict([image, caption])\n",
    "            index = out.argmax(-1)[0]\n",
    "            print(out.shape)\n",
    "            print(out)\n",
    "            #index = index[0]\n",
    "            word = index_to_word[index]\n",
    "            sentence.append(word)\n",
    "            count+= 1\n",
    "            if count >= max_caption_len or index == 0: #max caption length reach of '<eos>' encountered\n",
    "                break\n",
    "            caption[0,count] = index\n",
    "        sent_str = \" \".join(sentence)\n",
    "        print(\"The Oracle says : %s\" %sent_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images\n",
      "Loaded captions\n",
      "Loaded vocabulary\n"
     ]
    }
   ],
   "source": [
    "d_images=load_images()\n",
    "print('Loaded images')\n",
    "d_captions=load_captions()\n",
    "print('Loaded captions')\n",
    "vocab, vocab2 = load_vocabulary()\n",
    "print('Loaded vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtf next_word_index 1 for next_word ,\n",
      " wtf index 1 for word ,\n",
      "wtf next_word_index 1 for next_word ,\n",
      " wtf index 1 for word ,\n",
      "wtf next_word_index 1 for next_word ,\n",
      " wtf index 1 for word ,\n",
      "Loaded images, partial_captions, next_words\n",
      "Training now\n",
      "Loading weights\n",
      "Loaded weights\n",
      "Built Image Model\n",
      "Building Language Model\n",
      "Adding Embedding\n",
      "Adding LSTM\n",
      "Adding TimeDistributed Dense\n",
      "Built model.\n",
      "Compiling Now\n",
      "Fitting Now\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 160s - loss: 9.6718 - acc: 0.0160   \n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 137s - loss: 7.5391 - acc: 0.0800   \n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 132s - loss: 5.4355 - acc: 0.0800   \n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 130s - loss: 4.2085 - acc: 0.0480   \n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 132s - loss: 3.9037 - acc: 0.0960   \n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 135s - loss: 3.8158 - acc: 0.0960   \n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 132s - loss: 3.7648 - acc: 0.0960   \n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 131s - loss: 3.7598 - acc: 0.0800   \n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 131s - loss: 3.7544 - acc: 0.0800   \n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 135s - loss: 3.7579 - acc: 0.0480   \n",
      "Predicting now\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n",
      "(1, 16)\n",
      "(1, 23117)\n",
      "[[  1.00910589e-01   3.21085416e-02   9.47804973e-02 ...,   7.87889808e-02\n",
      "    6.32909007e-07   6.94503001e-07]]\n",
      "The Oracle says : <eos>\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "images, partial_captions, next_words = gen_image_partial_captions(d_images, d_captions, vocab)\n",
    "print('Loaded images, partial_captions, next_words')\n",
    "print('Training now')\n",
    "\n",
    "\n",
    "model = train(images, partial_captions, next_words, vocab_size)\n",
    "#timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#print(model.summary())\n",
    "print(\"Predicting now\")\n",
    "predict(model, d_images, vocab2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
