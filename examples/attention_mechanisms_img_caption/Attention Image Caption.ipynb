{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import flickr30k\n",
    "import numpy as numpy\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from collections import OrderedDict\n",
    "import cPickle as pkl\n",
    "import numpy\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from sklearn.cross_validation import KFold\n",
    "import warnings\n",
    "# [see Section (4.3) for explanation]\n",
    "from homogeneous_data import HomogeneousData\n",
    "# supported optimizers\n",
    "from optimizers import adadelta, adam, rmsprop, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... loaded train\n",
      "... loaded test\n",
      "... loaded dev\n"
     ]
    }
   ],
   "source": [
    "train, valid, test, worddict = flickr30k.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some useful shorthands\n",
    "def tanh(x):\n",
    "    return tensor.tanh(x)\n",
    "\n",
    "def rectifier(x):\n",
    "    return tensor.maximum(0., x)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index 0 and 1 always code for the end of sentence and unknown token\n",
    "word_idict = dict()\n",
    "for kk, vv in worddict.iteritems():\n",
    "    word_idict[vv] = kk\n",
    "word_idict[0] = '<eos>'\n",
    "word_idict[1] = 'UNK'\n",
    "\n",
    "\n",
    "layers = {'ff': ('param_init_fflayer', 'fflayer'),\n",
    "          'lstm': ('param_init_lstm', 'lstm_layer'),\n",
    "          'lstm_cond': ('param_init_lstm_cond', 'lstm_cond_layer'),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prefix-appended name\n",
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)\n",
    "\n",
    "# initialize Theano shared variables according to the initial parameters\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.iteritems():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return (eval(fns[0]), eval(fns[1]))\n",
    "\n",
    "# feedforward layer: affine transformation + point-wise nonlinearity\n",
    "def param_init_fflayer(options, params, prefix='ff', nin=None, nout=None):\n",
    "    if nin is None:\n",
    "        nin = options['dim_proj']\n",
    "    if nout is None:\n",
    "        nout = options['dim_proj']\n",
    "    params[_p(prefix, 'W')] = norm_weight(nin, nout, scale=0.01)\n",
    "    params[_p(prefix, 'b')] = numpy.zeros((nout,)).astype('float32')\n",
    "\n",
    "    return params\n",
    "\n",
    "def fflayer(tparams, state_below, options, prefix='rconv', activ='lambda x: tensor.tanh(x)', **kwargs):\n",
    "    return eval(activ)(tensor.dot(state_below, tparams[_p(prefix,'W')])+tparams[_p(prefix,'b')])\n",
    "\n",
    "\n",
    "# Conditional LSTM layer with Attention\n",
    "def param_init_lstm_cond(options, params, prefix='lstm_cond', nin=None, dim=None, dimctx=None):\n",
    "    if nin is None:\n",
    "        nin = options['dim']\n",
    "    if dim is None:\n",
    "        dim = options['dim']\n",
    "    if dimctx is None:\n",
    "        dimctx = options['dim']\n",
    "    # input to LSTM, similar to the above, we stack the matricies for compactness, do one\n",
    "    # dot product, and use the slice function below to get the activations for each \"gate\"\n",
    "    W = numpy.concatenate([norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim),\n",
    "                           norm_weight(nin,dim)], axis=1)\n",
    "    params[_p(prefix,'W')] = W\n",
    "\n",
    "    # LSTM to LSTM\n",
    "    U = numpy.concatenate([ortho_weight(dim),\n",
    "                           ortho_weight(dim),\n",
    "                           ortho_weight(dim),\n",
    "                           ortho_weight(dim)], axis=1)\n",
    "    params[_p(prefix,'U')] = U\n",
    "\n",
    "    # bias to LSTM\n",
    "    params[_p(prefix,'b')] = numpy.zeros((4 * dim,)).astype('float32')\n",
    "\n",
    "    # context to LSTM\n",
    "    Wc = norm_weight(dimctx,dim*4)\n",
    "    params[_p(prefix,'Wc')] = Wc\n",
    "\n",
    "    # attention: context -> hidden\n",
    "    Wc_att = norm_weight(dimctx, ortho=False)\n",
    "    params[_p(prefix,'Wc_att')] = Wc_att\n",
    "\n",
    "    # attention: LSTM -> hidden\n",
    "    Wd_att = norm_weight(dim,dimctx)\n",
    "    params[_p(prefix,'Wd_att')] = Wd_att\n",
    "\n",
    "    # attention: hidden bias\n",
    "    b_att = numpy.zeros((dimctx,)).astype('float32')\n",
    "    params[_p(prefix,'b_att')] = b_att\n",
    "\n",
    "    # attention:\n",
    "    U_att = norm_weight(dimctx,1)\n",
    "    params[_p(prefix,'U_att')] = U_att\n",
    "    c_att = numpy.zeros((1,)).astype('float32')\n",
    "    params[_p(prefix, 'c_tt')] = c_att\n",
    "\n",
    "    return params\n",
    "\n",
    "def lstm_cond_layer(tparams, state_below, options, prefix='lstm_cond',\n",
    "                    mask=None, context=None, one_step=False,\n",
    "                    init_memory=None, init_state=None,\n",
    "                    trng=None, use_noise=None, sampling=True,\n",
    "                    argmax=False, **kwargs):\n",
    "\n",
    "    assert context, 'Context must be provided'\n",
    "\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    # mask\n",
    "    if mask is None:\n",
    "        mask = tensor.alloc(1., state_below.shape[0], 1)\n",
    "\n",
    "    # infer lstm dimension\n",
    "    dim = tparams[_p(prefix, 'U')].shape[0]\n",
    "\n",
    "    # initial/previous state\n",
    "    if init_state is None:\n",
    "        init_state = tensor.alloc(0., n_samples, dim)\n",
    "    # initial/previous memory\n",
    "    if init_memory is None:\n",
    "        init_memory = tensor.alloc(0., n_samples, dim)\n",
    "\n",
    "    # projected context\n",
    "    pctx_ = tensor.dot(context, tparams[_p(prefix,'Wc_att')]) + tparams[_p(prefix, 'b_att')]\n",
    "\n",
    "    # projected x\n",
    "    # state_below is timesteps*num samples by d in training (TODO change to notation of paper)\n",
    "    # this is n * d during sampling\n",
    "    state_below = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n*dim:(n+1)*dim]\n",
    "        return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "    def _step(m_, x_, h_, c_, a_, as_, ct_, pctx_, dp_=None, dp_att_=None):\n",
    "        \"\"\" Each variable is one time slice of the LSTM\n",
    "        m_ - (mask), x_- (previous word), h_- (hidden state), c_- (lstm memory),\n",
    "        a_ - (alpha distribution [eq (5)]), as_- (sample from alpha dist), ct_- (context), \n",
    "        pctx_ (projected context), dp_/dp_att_ (dropout masks)\n",
    "        \"\"\"\n",
    "        # attention computation\n",
    "        # [described in  equations (4), (5), (6) in\n",
    "        # section \"3.1.2 Decoder: Long Short Term Memory Network]\n",
    "        pstate_ = tensor.dot(h_, tparams[_p(prefix,'Wd_att')])\n",
    "        pctx_ = pctx_ + pstate_[:,None,:]\n",
    "        pctx_list = []\n",
    "        pctx_list.append(pctx_)\n",
    "        pctx_ = tanh(pctx_)\n",
    "        alpha = tensor.dot(pctx_, tparams[_p(prefix,'U_att')])+tparams[_p(prefix, 'c_tt')]\n",
    "        alpha_pre = alpha\n",
    "        alpha_shp = alpha.shape\n",
    "\n",
    "        #SOFT ATTENTION ONLY #\n",
    "        alpha = tensor.nnet.softmax(alpha.reshape([alpha_shp[0],alpha_shp[1]])) # softmax\n",
    "        ctx_ = (context * alpha[:,:,None]).sum(1) # current context\n",
    "        alpha_sample = alpha # you can return something else reasonable here to debug\n",
    "\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')])\n",
    "        preact += x_\n",
    "        preact += tensor.dot(ctx_, tparams[_p(prefix, 'Wc')])\n",
    "\n",
    "        # Recover the activations to the lstm gates\n",
    "        # [equation (1)]\n",
    "        i = _slice(preact, 0, dim)\n",
    "        f = _slice(preact, 1, dim)\n",
    "        o = _slice(preact, 2, dim)\n",
    "\n",
    "        i = tensor.nnet.sigmoid(i)\n",
    "        f = tensor.nnet.sigmoid(f)\n",
    "        o = tensor.nnet.sigmoid(o)\n",
    "        c = tensor.tanh(_slice(preact, 3, dim))\n",
    "\n",
    "        # compute the new memory/hidden state\n",
    "        # if the mask is 0, just copy the previous state\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:,None] * c + (1. - m_)[:,None] * c_ \n",
    "\n",
    "        h = o * tensor.tanh(c)\n",
    "        h = m_[:,None] * h + (1. - m_)[:,None] * h_\n",
    "\n",
    "        rval = [h, c, alpha, alpha_sample, ctx_]\n",
    "        if options['selector']:\n",
    "            rval += [sel_]\n",
    "        rval += [pstate_, pctx_, i, f, o, preact, alpha_pre]+pctx_list\n",
    "        return rval\n",
    "\n",
    "    \n",
    "    _step0 = lambda m_, x_, h_, c_, a_, as_, ct_, pctx_: _step(m_, x_, h_, c_, a_, as_, ct_, pctx_)\n",
    "\n",
    "\n",
    "    seqs = [mask, state_below]\n",
    "    \n",
    "    rval, updates = theano.scan(_step0,\n",
    "                                sequences=seqs,\n",
    "                                outputs_info=outputs_info,\n",
    "                                non_sequences=[pctx_],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps, profile=False)\n",
    "    return rval, updates\n",
    "\n",
    "def validate_options(options):\n",
    "    # Put friendly reminders here\n",
    "    if options['dim_word'] > options['dim']:\n",
    "        warnings.warn('dim_word should only be as large as dim.')\n",
    "\n",
    "    if options['lstm_encoder']:\n",
    "        warnings.warn('Note that this is a 1-D bidirectional LSTM, not 2-D one.')\n",
    "\n",
    "    if options['use_dropout_lstm']:\n",
    "        warnings.warn('dropout in the lstm seems not to help')\n",
    "\n",
    "    # Other checks:\n",
    "    if options['attn_type'] not in ['stochastic', 'deterministic']:\n",
    "        raise ValueError(\"specified attention type is not correct\")\n",
    "\n",
    "    return options\n",
    "\n",
    "# some utilities\n",
    "def ortho_weight(ndim):\n",
    "    \"\"\"\n",
    "    Random orthogonal weights\n",
    "\n",
    "    Used by norm_weights(below), in which case, we\n",
    "    are ensuring that the rows are orthogonal\n",
    "    (i.e W = U \\Sigma V, U has the same\n",
    "    # of rows, V has the same # of cols)\n",
    "    \"\"\"\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, _, _ = numpy.linalg.svd(W)\n",
    "    return u.astype('float32')\n",
    "\n",
    "def norm_weight(nin,nout=None, scale=0.01, ortho=True):\n",
    "    \"\"\"\n",
    "    Random weights drawn from a Gaussian\n",
    "    \"\"\"\n",
    "    if nout is None:\n",
    "        nout = nin\n",
    "    if nout == nin and ortho:\n",
    "        W = ortho_weight(nin)\n",
    "    else:\n",
    "        W = scale * numpy.random.randn(nin, nout)\n",
    "    return W.astype('float32')\n",
    "\n",
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "    # embedding: [matrix E in paper]\n",
    "    params['Wemb'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "    ctx_dim = options['ctx_dim']\n",
    "    \n",
    "    # init_state, init_cell: [top right on page 4]\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_state', nin=ctx_dim, nout=options['dim'])\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_memory', nin=ctx_dim, nout=options['dim'])\n",
    "    # decoder: LSTM: [equation (1)/(2)/(3)]\n",
    "    params = get_layer('lstm_cond')[0](options, params, prefix='decoder',\n",
    "                                       nin=options['dim_word'], dim=options['dim'],\n",
    "                                       dimctx=ctx_dim)\n",
    "    \n",
    "    # readout: [equation (7)]\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_lstm', nin=options['dim'], nout=options['dim_word'])\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit', nin=options['dim_word'], nout=options['n_words'])\n",
    "\n",
    "    return params\n",
    "\n",
    "# build a training model\n",
    "def build_model(tparams, options, sampling=True):\n",
    "    \"\"\" Builds the entire computational graph used for training\n",
    "\n",
    "    [This function builds a model described in Section 3.1.2 onwards\n",
    "    as the convolutional feature are precomputed, some extra features\n",
    "    which were not used are also implemented here.]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tparams : OrderedDict\n",
    "        maps names of variables to theano shared variables\n",
    "    options : dict\n",
    "        big dictionary with all the settings and hyperparameters\n",
    "    sampling : boolean\n",
    "        [If it is true, when using stochastic attention, follows\n",
    "        the learning rule described in section 4. at the bottom left of\n",
    "        page 5]\n",
    "    Returns\n",
    "    -------\n",
    "    trng: theano random number generator\n",
    "        Used for dropout, stochastic attention, etc\n",
    "    use_noise: theano shared variable\n",
    "        flag that toggles noise on and off\n",
    "    [x, mask, ctx]: theano variables\n",
    "        Represent the captions, binary mask, and annotations\n",
    "        for a single batch (see dimensions below)\n",
    "    alphas: theano variables\n",
    "        Attention weights\n",
    "    alpha_sample: theano variable\n",
    "        Sampled attention weights used in REINFORCE for stochastic\n",
    "        attention: [see the learning rule in eq (12)]\n",
    "    cost: theano variable\n",
    "        negative log likelihood\n",
    "    opt_outs: OrderedDict\n",
    "        extra outputs required depending on configuration in options\n",
    "    \"\"\"\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy.float32(0.))\n",
    "\n",
    "    # description string: #words x #samples,\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    mask = tensor.matrix('mask', dtype='float32')\n",
    "    # context: #samples x #annotations x dim\n",
    "    ctx = tensor.tensor3('ctx', dtype='float32')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    # index into the word embedding matrix, shift it forward in time\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps, n_samples, options['dim_word']])\n",
    "    emb_shifted = tensor.zeros_like(emb)\n",
    "    emb_shifted = tensor.set_subtensor(emb_shifted[1:], emb[:-1])\n",
    "    emb = emb_shifted\n",
    "\n",
    "    ctx0 = ctx\n",
    "\n",
    "    # initial state/cell [top right on page 4]\n",
    "    ctx_mean = ctx0.mean(1)\n",
    "    #### FROM PAPER ######### C_0\n",
    "    init_state = get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_state', activ='tanh')\n",
    "    #### FROM PAPER ######### H_0\n",
    "    init_memory = get_layer('ff')[1](tparams, ctx_mean, options, prefix='ff_memory', activ='tanh')\n",
    "    # lstm decoder\n",
    "    # [equation (1), (2), (3) in section 3.1.2]\n",
    "    attn_updates = []\n",
    "\n",
    "    proj, updates = get_layer('lstm_cond')[1](tparams, emb, options,prefix='decoder',mask=mask, context=ctx0,one_step=False,init_state=init_state,init_memory=init_memory,trng=trng,use_noise=use_noise,sampling=sampling)\n",
    "    attn_updates += updates\n",
    "    proj_h = proj[0]\n",
    "\n",
    "    alphas = proj[2]\n",
    "    alpha_sample = proj[3]\n",
    "    ctxs = proj[4]\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        proj_h = dropout_layer(proj_h, use_noise, trng)\n",
    "\n",
    "    # compute word probabilities\n",
    "    # [equation (7)]\n",
    "    logit = get_layer('ff')[1](tparams, proj_h, options, prefix='ff_logit_lstm', activ='linear')\n",
    "    #if options['prev2out']:\n",
    "    #    logit += emb\n",
    "    #if options['ctx2out']:\n",
    "    #    logit += get_layer('ff')[1](tparams, ctxs, options, prefix='ff_logit_ctx', activ='linear')\n",
    "    logit = tanh(logit)\n",
    "    if options['use_dropout']:\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    # compute softmax\n",
    "    logit = get_layer('ff')[1](tparams, logit, options, prefix='ff_logit', activ='linear')\n",
    "    logit_shp = logit.shape\n",
    "    probs = tensor.nnet.softmax(logit.reshape([logit_shp[0]*logit_shp[1], logit_shp[2]]))\n",
    "\n",
    "    # Index into the computed probability to give the log likelihood\n",
    "    x_flat = x.flatten()\n",
    "    p_flat = probs.flatten()\n",
    "    cost = -tensor.log(p_flat[tensor.arange(x_flat.shape[0])*probs.shape[1]+x_flat]+1e-8)\n",
    "    cost = cost.reshape([x.shape[0], x.shape[1]])\n",
    "    masked_cost = cost * mask\n",
    "    cost = (masked_cost).sum(0)\n",
    "\n",
    "    # optional outputs\n",
    "    opt_outs = dict() \n",
    "\n",
    "    if options['attn_type'] == 'stochastic':\n",
    "        opt_outs['masked_cost'] = masked_cost # need this for reinforce later\n",
    "        opt_outs['attn_updates'] = attn_updates # this is to update the rng\n",
    "\n",
    "    return trng, use_noise, [x, mask, ctx], alphas, alpha_sample, cost, opt_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dim_word=100,  # word vector dimensionality\n",
    "          ctx_dim=512,  # context vector dimensionality\n",
    "          dim=1000,  # the number of LSTM units\n",
    "          attn_type='deterministic',  # [see section 4 from paper]\n",
    "          n_layers_att=1,  # number of layers used to compute the attention weights\n",
    "          n_layers_out=1,  # number of layers used to compute logit\n",
    "          n_layers_lstm=1,  # number of lstm layers - NOT USING DEEP DECODER\n",
    "          n_layers_init=1,  # number of layers to initialize LSTM at time 0\n",
    "          lstm_encoder=False,  # if True, run bidirectional LSTM on input units\n",
    "          prev2out=False,  # Feed previous word into logit\n",
    "          ctx2out=False,  # Feed attention weighted ctx into logit\n",
    "          alpha_entropy_c=0.002,  # hard attn param\n",
    "          RL_sumCost=True,  # hard attn param\n",
    "          semi_sampling_p=0.5,  # hard attn param\n",
    "          temperature=1.,  # hard attn param\n",
    "          patience=10,\n",
    "          max_epochs=5000,\n",
    "          dispFreq=100,\n",
    "          decay_c=0.,  # weight decay coeff\n",
    "          alpha_c=0.,  # doubly stochastic coeff\n",
    "          lrate=0.01,  # used only for SGD\n",
    "          selector=False,  # selector (see paper)\n",
    "          n_words=23112,  # vocab size for flickr30k\n",
    "          maxlen=100,  # maximum length of the description\n",
    "          optimizer='rmsprop',\n",
    "          batch_size = 16,\n",
    "          valid_batch_size = 16,\n",
    "          saveto='model.npz',  # relative path of saved model file\n",
    "          validFreq=1000,\n",
    "          saveFreq=1000,  # save the parameters after every saveFreq updates\n",
    "          sampleFreq=100,  # generate some samples after every sampleFreq updates\n",
    "          dataset='flickr30k',\n",
    "          dictionary=None,  # word dictionary\n",
    "          use_dropout=False,  # setting this true turns on dropout at various points\n",
    "          use_dropout_lstm=False,  # dropout on lstm gates\n",
    "          reload_=False,\n",
    "          save_per_epoch=False):\n",
    "    model_opts = locals().copy()\n",
    "    model_opts = validate_options(model_opts)\n",
    "    print model_opts\n",
    "    print 'Building model'\n",
    "    params = init_params(model_opts)\n",
    "    tparams = init_tparams(params)\n",
    "    # In order, we get:\n",
    "    #   1) trng - theano random number generator\n",
    "    #   2) use_noise - flag that turns on dropout\n",
    "    #   3) inps - inputs for f_grad_shared\n",
    "    #   4) cost - log likelihood for each sentence\n",
    "    #   5) opts_out - optional outputs (e.g selector)\n",
    "    trng, use_noise, inps, alphas, alphas_sample, cost, opt_outs = build_model(tparams, model_opts)\n",
    "    print 'Buliding sampler'\n",
    "    f_init, f_next = build_sampler(tparams, model_opts, use_noise, trng)\n",
    "\n",
    "    # we want the cost without any the regularizers\n",
    "    f_log_probs = theano.function(inps, -cost, profile=False,\n",
    "                                        updates=opt_outs['attn_updates']\n",
    "                                        if model_opts['attn_type']=='stochastic'\n",
    "                                        else None)\n",
    "\n",
    "    cost = cost.mean()\n",
    "    # add L2 regularization costs\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy.float32(decay_c), name='decay_c')\n",
    "        weight_decay = 0.\n",
    "        for kk, vv in tparams.iteritems():\n",
    "            weight_decay += (vv ** 2).sum()\n",
    "        weight_decay *= decay_c\n",
    "        cost += weight_decay\n",
    "\n",
    "    # Doubly stochastic regularization\n",
    "    if alpha_c > 0.:\n",
    "        alpha_c = theano.shared(numpy.float32(alpha_c), name='alpha_c')\n",
    "        alpha_reg = alpha_c * ((1.-alphas.sum(0))**2).sum(0).mean()\n",
    "        cost += alpha_reg\n",
    "\n",
    "    hard_attn_updates = []\n",
    "    # Backprop!\n",
    "    \n",
    "    grads = tensor.grad(cost, wrt=itemlist(tparams))\n",
    "    \n",
    "    # to getthe cost after regularization or the gradients, use this\n",
    "    # f_cost = theano.function([x, mask, ctx], cost, profile=False)\n",
    "    # f_grad = theano.function([x, mask, ctx], grads, profile=False)\n",
    "\n",
    "    # f_grad_shared computes the cost and updates adaptive learning rate variables\n",
    "    # f_update updates the weights of the model\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    f_grad_shared, f_update = eval(optimizer)(lr, tparams, grads, inps, cost, hard_attn_updates)\n",
    "\n",
    "    print 'Optimization'\n",
    "\n",
    "    # [See note in section 4.3 of paper]\n",
    "    train_iter = HomogeneousData(train, batch_size=batch_size, maxlen=maxlen)\n",
    "\n",
    "    if valid:\n",
    "        kf_valid = KFold(len(valid[0]), n_folds=len(valid[0])/valid_batch_size, shuffle=False)\n",
    "    if test:\n",
    "        kf_test = KFold(len(test[0]), n_folds=len(test[0])/valid_batch_size, shuffle=False)\n",
    "\n",
    "    # history_errs is a bare-bones training log that holds the validation and test error\n",
    "    history_errs = []\n",
    "    # reload history\n",
    "    if reload_ and os.path.exists(saveto):\n",
    "        history_errs = numpy.load(saveto)['history_errs'].tolist()\n",
    "    best_p = None\n",
    "    bad_counter = 0\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = len(train[0])/batch_size\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = len(train[0])/batch_size\n",
    "    if sampleFreq == -1:\n",
    "        sampleFreq = len(train[0])/batch_size\n",
    "\n",
    "    uidx = 0\n",
    "    estop = False\n",
    "    for eidx in xrange(max_epochs):\n",
    "        n_samples = 0\n",
    "\n",
    "        print 'Epoch ', eidx\n",
    "\n",
    "        for caps in train_iter:\n",
    "            n_samples += len(caps)\n",
    "            uidx += 1\n",
    "            # turn on dropout\n",
    "            use_noise.set_value(1.)\n",
    "\n",
    "            # preprocess the caption, recording the\n",
    "            # time spent to help detect bottlenecks\n",
    "            pd_start = time.time()\n",
    "            x, mask, ctx = prepare_data(caps,\n",
    "                                        train[1],\n",
    "                                        worddict,\n",
    "                                        maxlen=maxlen,\n",
    "                                        n_words=n_words)\n",
    "            pd_duration = time.time() - pd_start\n",
    "\n",
    "            if x is None:\n",
    "                print 'Minibatch with zero sample under length ', maxlen\n",
    "                continue\n",
    "\n",
    "            # get the cost for the minibatch, and update the weights\n",
    "            ud_start = time.time()\n",
    "            cost = f_grad_shared(x, mask, ctx)\n",
    "            f_update(lrate)\n",
    "            ud_duration = time.time() - ud_start # some monitoring for each mini-batch\n",
    "\n",
    "            # Numerical stability check\n",
    "            if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                print 'NaN detected'\n",
    "                return 1., 1., 1.\n",
    "\n",
    "            if numpy.mod(uidx, dispFreq) == 0:\n",
    "                print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost, 'PD ', pd_duration, 'UD ', ud_duration\n",
    "\n",
    "            # Checkpoint\n",
    "            if numpy.mod(uidx, saveFreq) == 0:\n",
    "                print 'Saving...',\n",
    "\n",
    "                if best_p is not None:\n",
    "                    params = copy.copy(best_p)\n",
    "                else:\n",
    "                    params = unzip(tparams)\n",
    "                numpy.savez(saveto, history_errs=history_errs, **params)\n",
    "                pkl.dump(model_opts, open('%s.pkl'%saveto, 'wb'))\n",
    "                print 'Done'\n",
    "\n",
    "            # Print a generated sample as a sanity check\n",
    "            if numpy.mod(uidx, sampleFreq) == 0:\n",
    "                # turn off dropout first\n",
    "                use_noise.set_value(0.)\n",
    "                x_s = x\n",
    "                mask_s = mask\n",
    "                ctx_s = ctx\n",
    "                # generate and decode the a subset of the current training batch\n",
    "                for jj in xrange(numpy.minimum(10, len(caps))):\n",
    "                    sample, score = gen_sample(tparams, f_init, f_next, ctx_s[jj], model_opts,\n",
    "                                               trng=trng, k=5, maxlen=30, stochastic=False)\n",
    "                    # Decode the sample from encoding back to words\n",
    "                    print 'Truth ',jj,': ',\n",
    "                    for vv in x_s[:,jj]:\n",
    "                        if vv == 0:\n",
    "                            break\n",
    "                        if vv in word_idict:\n",
    "                            print word_idict[vv],\n",
    "                        else:\n",
    "                            print 'UNK',\n",
    "                    print\n",
    "                    for kk, ss in enumerate([sample[0]]):\n",
    "                        print 'Sample (', kk,') ', jj, ': ',\n",
    "                        for vv in ss:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in word_idict:\n",
    "                                print word_idict[vv],\n",
    "                            else:\n",
    "                                print 'UNK',\n",
    "                    print\n",
    "\n",
    "            # Log validation loss + checkpoint the model with the best validation log likelihood\n",
    "            if numpy.mod(uidx, validFreq) == 0:\n",
    "                use_noise.set_value(0.)\n",
    "                train_err = 0\n",
    "                valid_err = 0\n",
    "                test_err = 0\n",
    "\n",
    "                if valid:\n",
    "                    valid_err = -pred_probs(f_log_probs, model_opts, worddict, prepare_data, valid, kf_valid).mean()\n",
    "                if test:\n",
    "                    test_err = -pred_probs(f_log_probs, model_opts, worddict, prepare_data, test, kf_test).mean()\n",
    "\n",
    "                history_errs.append([valid_err, test_err])\n",
    "\n",
    "                # the model with the best validation long likelihood is saved seperately with a different name\n",
    "                if uidx == 0 or valid_err <= numpy.array(history_errs)[:,0].min():\n",
    "                    best_p = unzip(tparams)\n",
    "                    print 'Saving model with best validation ll'\n",
    "                    params = copy.copy(best_p)\n",
    "                    params = unzip(tparams)\n",
    "                    numpy.savez(saveto+'_bestll', history_errs=history_errs, **params)\n",
    "                    bad_counter = 0\n",
    "\n",
    "                # abort training if perplexity has been increasing for too long\n",
    "                if eidx > patience and len(history_errs) > patience and valid_err >= numpy.array(history_errs)[:-patience,0].min():\n",
    "                    bad_counter += 1\n",
    "                    if bad_counter > patience:\n",
    "                        print 'Early Stop!'\n",
    "                        estop = True\n",
    "                        break\n",
    "\n",
    "                print 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err\n",
    "\n",
    "        print 'Seen %d samples' % n_samples\n",
    "\n",
    "        if estop:\n",
    "            break\n",
    "\n",
    "        if save_per_epoch:\n",
    "            numpy.savez(saveto + '_epoch_' + str(eidx + 1), history_errs=history_errs, **unzip(tparams))\n",
    "\n",
    "    # use the best nll parameters for final checkpoint (if they exist)\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    train_err = 0\n",
    "    valid_err = 0\n",
    "    test_err = 0\n",
    "    if valid:\n",
    "        valid_err = -pred_probs(f_log_probs, model_opts, worddict, prepare_data, valid, kf_valid)\n",
    "    if test:\n",
    "        test_err = -pred_probs(f_log_probs, model_opts, worddict, prepare_data, test, kf_test)\n",
    "\n",
    "    print 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err\n",
    "\n",
    "    params = copy.copy(best_p)\n",
    "    numpy.savez(saveto, zipped_params=best_p, train_err=train_err,\n",
    "                valid_err=valid_err, test_err=test_err, history_errs=history_errs,\n",
    "                **params)\n",
    "\n",
    "    return train_err, valid_err, test_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lrate': 0.01, 'decay_c': 0.0, 'patience': 10, 'save_per_epoch': False, 'n_layers_init': 1, 'RL_sumCost': True, 'max_epochs': 5000, 'dispFreq': 100, 'attn_type': 'deterministic', 'alpha_c': 0.0, 'temperature': 1.0, 'n_layers_att': 1, 'saveto': 'model.npz', 'ctx_dim': 512, 'valid_batch_size': 16, 'lstm_encoder': False, 'n_layers_lstm': 1, 'optimizer': 'rmsprop', 'validFreq': 1000, 'dictionary': None, 'batch_size': 16, 'selector': False, 'n_words': 23112, 'dataset': 'flickr30k', 'use_dropout_lstm': False, 'prev2out': False, 'dim': 1000, 'use_dropout': False, 'dim_word': 100, 'sampleFreq': 100, 'semi_sampling_p': 0.5, 'n_layers_out': 1, 'saveFreq': 1000, 'maxlen': 100, 'alpha_entropy_c': 0.002, 'ctx2out': False, 'reload_': False}\n",
      "Building model\n",
      "Buliding sampler\n",
      "Building f_init... Done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('An update must have the same type as the original shared variable (shared_var=Wemb_rgrad, shared_var.type=TensorType(float32, matrix), update_val=Elemwise{add,no_inplace}.0, update_val.type=TensorType(float64, matrix)).', 'If the difference is related to the broadcast pattern, you can call the tensor.unbroadcast(var, axis_to_unbroadcast[, ...]) function to remove broadcastable dimensions.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-aa1958cd27a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dim_word, ctx_dim, dim, attn_type, n_layers_att, n_layers_out, n_layers_lstm, n_layers_init, lstm_encoder, prev2out, ctx2out, alpha_entropy_c, RL_sumCost, semi_sampling_p, temperature, patience, max_epochs, dispFreq, decay_c, alpha_c, lrate, selector, n_words, maxlen, optimizer, batch_size, valid_batch_size, saveto, validFreq, saveFreq, sampleFreq, dataset, dictionary, use_dropout, use_dropout_lstm, reload_, save_per_epoch)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# f_update updates the weights of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mf_grad_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_attn_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Optimization'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/github/image_caption_using_attention/attention_mechanisms_img_caption/optimizers.py\u001b[0m in \u001b[0;36mrmsprop\u001b[0;34m(lr, tparams, grads, inp, cost, hard_attn_up)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mrg2up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrg2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_grads2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mf_grad_shared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzgup\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrgup\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrg2up\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhard_attn_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mupdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_updir'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    447\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0;31m# extracting the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloned_extended_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[0;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[1;32m    206\u001b[0m                        ' function to remove broadcastable dimensions.')\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_sug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mupdate_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstore_into\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('An update must have the same type as the original shared variable (shared_var=Wemb_rgrad, shared_var.type=TensorType(float32, matrix), update_val=Elemwise{add,no_inplace}.0, update_val.type=TensorType(float64, matrix)).', 'If the difference is related to the broadcast pattern, you can call the tensor.unbroadcast(var, axis_to_unbroadcast[, ...]) function to remove broadcastable dimensions.')"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lrate': 0.01, 'decay_c': 0.0, 'patience': 10, 'save_per_epoch': False, 'n_layers_init': 1, 'RL_sumCost': True, 'max_epochs': 5000, 'dispFreq': 100, 'attn_type': 'deterministic', 'alpha_c': 0.0, 'temperature': 1.0, 'n_layers_att': 1, 'saveto': 'model.npz', 'ctx_dim': 512, 'valid_batch_size': 16, 'lstm_encoder': False, 'n_layers_lstm': 1, 'optimizer': 'rmsprop', 'validFreq': 1000, 'dictionary': None, 'batch_size': 16, 'selector': False, 'n_words': 23112, 'dataset': 'flickr30k', 'use_dropout_lstm': False, 'prev2out': False, 'dim': 1000, 'use_dropout': False, 'dim_word': 100, 'sampleFreq': 100, 'semi_sampling_p': 0.5, 'n_layers_out': 1, 'saveFreq': 1000, 'maxlen': 100, 'alpha_entropy_c': 0.002, 'ctx2out': False, 'reload_': False}\n",
      "Building model\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun(dim_word=100,  # word vector dimensionality\n",
    "          ctx_dim=512,  # context vector dimensionality\n",
    "          dim=1000,  # the number of LSTM units\n",
    "          attn_type='deterministic',  # [see section 4 from paper]\n",
    "          n_layers_att=1,  # number of layers used to compute the attention weights\n",
    "          n_layers_out=1,  # number of layers used to compute logit\n",
    "          n_layers_lstm=1,  # number of lstm layers - NOT USING DEEP DECODER\n",
    "          n_layers_init=1,  # number of layers to initialize LSTM at time 0\n",
    "          lstm_encoder=False,  # if True, run bidirectional LSTM on input units\n",
    "          prev2out=False,  # Feed previous word into logit\n",
    "          ctx2out=False,  # Feed attention weighted ctx into logit\n",
    "          alpha_entropy_c=0.002,  # hard attn param\n",
    "          RL_sumCost=True,  # hard attn param\n",
    "          semi_sampling_p=0.5,  # hard attn param\n",
    "          temperature=1.,  # hard attn param\n",
    "          patience=10,\n",
    "          max_epochs=5000,\n",
    "          dispFreq=100,\n",
    "          decay_c=0.,  # weight decay coeff\n",
    "          alpha_c=0.,  # doubly stochastic coeff\n",
    "          lrate=0.01,  # used only for SGD\n",
    "          selector=False,  # selector (see paper)\n",
    "          n_words=23112,  # vocab size for flickr30k\n",
    "          maxlen=100,  # maximum length of the description\n",
    "          optimizer='rmsprop',\n",
    "          batch_size = 16,\n",
    "          valid_batch_size = 16,\n",
    "          saveto='model.npz',  # relative path of saved model file\n",
    "          validFreq=1000,\n",
    "          saveFreq=1000,  # save the parameters after every saveFreq updates\n",
    "          sampleFreq=100,  # generate some samples after every sampleFreq updates\n",
    "          dataset='flickr30k',\n",
    "          dictionary=None,  # word dictionary\n",
    "          use_dropout=False,  # setting this true turns on dropout at various points\n",
    "          use_dropout_lstm=False,  # dropout on lstm gates\n",
    "          reload_=False,\n",
    "          save_per_epoch=False):\n",
    "    opts = locals().copy()\n",
    "    validate_options(opts)\n",
    "    print opts\n",
    "    print 'Building model'\n",
    "    params = init_params(opts)\n",
    "    tparams = init_tparams(params)\n",
    "    return params,tparams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lrate': 0.01, 'decay_c': 0.0, 'patience': 10, 'save_per_epoch': False, 'n_layers_init': 1, 'RL_sumCost': True, 'max_epochs': 5000, 'dispFreq': 100, 'attn_type': 'deterministic', 'alpha_c': 0.0, 'temperature': 1.0, 'n_layers_att': 1, 'saveto': 'model.npz', 'ctx_dim': 512, 'valid_batch_size': 16, 'lstm_encoder': False, 'n_layers_lstm': 1, 'optimizer': 'rmsprop', 'validFreq': 1000, 'dictionary': None, 'batch_size': 16, 'selector': False, 'n_words': 23112, 'dataset': 'flickr30k', 'use_dropout_lstm': False, 'prev2out': False, 'dim': 1000, 'use_dropout': False, 'dim_word': 100, 'sampleFreq': 100, 'semi_sampling_p': 0.5, 'n_layers_out': 1, 'saveFreq': 1000, 'maxlen': 100, 'alpha_entropy_c': 0.002, 'ctx2out': False, 'reload_': False}\n",
      "Building model\n"
     ]
    }
   ],
   "source": [
    "p,tp = fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.sharedvar.TensorSharedVariable"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tp['Wemb'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
